{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import os\n",
    "from os import makedirs\n",
    "from argparse import ArgumentParser\n",
    "from argparse import Namespace\n",
    "from arguments import ModelParams, PipelineParams, OptimizationParams, get_combined_args\n",
    "import numpy as np\n",
    "from scene import Scene, GaussianModel\n",
    "from utils.general_utils import safe_state\n",
    "import open3d as o3d\n",
    "from utils.loss_utils import l1_loss, ssim\n",
    "from utils.image_utils import psnr\n",
    "from random import randint\n",
    "from gaussian_renderer import render, network_gui\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.graphics_utils import BasicPointCloud\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from train_utils import showTensorDepthImage, showTensorImage, prepare_output_and_logger, training_report, showGSRender, loadPLY, saveTensorAsPLY, renderGS, saveCurrentRender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "# Set the precision of elements to 2 decimal places\n",
    "torch.set_printoptions(precision=2)\n",
    "\n",
    "# Limit the number of elements shown to 5\n",
    "torch.set_printoptions(threshold=5)\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showRasterizedImageComp(image1, image2):\n",
    "    # Create a figure with two subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Display the first image on the left subplot\n",
    "    axes[0].imshow(image1)\n",
    "    axes[0].set_title('Current PC Image')\n",
    "\n",
    "    # Display the second image on the right subplot\n",
    "    axes[1].imshow(image2)\n",
    "    axes[1].set_title('Global PC Raster')\n",
    "\n",
    "    # Hide the axes ticks\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()    \n",
    "\n",
    "def showRasterizedImage(u,v, colors, rgb_height=968, rgb_width=1296):\n",
    "    # print(u.shape, v.shape, colors.shape)\n",
    "    u = u.cpu().detach().numpy()\n",
    "    v = v.cpu().detach().numpy()\n",
    "    colors = colors.cpu().detach().numpy()\n",
    "\n",
    "    assert len(u) == len(v) == len(colors)\n",
    "    image_size = (rgb_height, rgb_width, 3)\n",
    "    raster = np.zeros(image_size, dtype=np.float32)\n",
    "\n",
    "    # print(np.shape(u), np.shape(v), np.shape(colors), np.shape(raster))\n",
    "\n",
    "    raster[v, u] = colors\n",
    "    # Clip values to ensure they are in the valid range [0, 1]\n",
    "    image = np.clip(raster, 0, 1)\n",
    "    plt.imshow(raster)\n",
    "    plt.show()\n",
    "    return image\n",
    "\n",
    "def showFilterMaskImage(indices_not_in_global, rgb_height=968, rgb_width=1296):\n",
    "    u = indices_not_in_global[:,0]\n",
    "    v = indices_not_in_global[:,1]\n",
    "    u = u.cpu().detach().numpy()\n",
    "    v = v.cpu().detach().numpy()\n",
    "    assert len(u) == len(v)\n",
    "    image_size = (rgb_height, rgb_width, 3)\n",
    "    raster = np.zeros(image_size, dtype=np.float32)\n",
    "    raster[v, u] = 1\n",
    "    plt.imshow(raster)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renderGSwithStats(viewpoint_cam, gaussians, pipeline):\n",
    "    with torch.no_grad():\n",
    "        print(viewpoint_cam.colmap_id)\n",
    "        print(viewpoint_cam.R)\n",
    "        print(viewpoint_cam.T)\n",
    "        print(viewpoint_cam.camera_center)\n",
    "        print(viewpoint_cam.image_width, viewpoint_cam.image_height)\n",
    "        print('W2C:',viewpoint_cam.world_view_transform.transpose(0,1))\n",
    "        print('Projection',viewpoint_cam.projection_matrix.transpose(0,1))\n",
    "        print('FProjection:',viewpoint_cam.full_proj_transform.transpose(0,1))\n",
    "\n",
    "        # Render\n",
    "        bg = torch.ones((3), device=\"cuda\")\n",
    "        render_pkg = render(viewpoint_cam, gaussians, pipeline, bg)\n",
    "        image, viewspace_point_tensor, visibility_filter, radii, raster_depth_map = render_pkg[\"render\"], render_pkg[\"viewspace_points\"], render_pkg[\"visibility_filter\"], render_pkg[\"radii\"], render_pkg[\"raster_depth\"]\n",
    "        # image, viewspace_point_tensor, visibility_filter, radii = render_pkg[\"render\"], render_pkg[\"viewspace_points\"], render_pkg[\"visibility_filter\"], render_pkg[\"radii\"]\n",
    "        showGSRender(image)\n",
    "\n",
    "        print(\"Visibility Filter: \", visibility_filter.shape)\n",
    "        print(\"Radii: \", radii.shape)\n",
    "        print(\"Viewsapce Points: \", viewspace_point_tensor.shape)\n",
    "    \n",
    "def getGaussianDepthMap(viewpoint_cam, gaussians, pipeline):\n",
    "    image, viewspace_point_tensor, visibility_filter, radii, raster_depth_map, visibility_map = renderGS(viewpoint_cam, gaussians, pipeline)\n",
    "    return raster_depth_map, visibility_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Command line Arg\n",
    "sys.argv = [\"train.py\", \n",
    "            \"-s\", \"G:\\\\Universitat Siegen\\\\SA\\\\P-GPU\\\\Code\\\\gaussian-splatting\\\\data\\\\RGBD_Data\\\\SyntheticV1\",\n",
    "            \"-r\" ,\"1\",\n",
    "            \"-m\" , \"G:\\\\Universitat Siegen\\\\SA\\\\P-GPU\\\\Code\\\\gaussian-splatting\\\\output\\\\SyntheticV1\"]\n",
    "\n",
    "parser = ArgumentParser(description='Progressice Gaussian Splatting')\n",
    "model = ModelParams(parser)\n",
    "pipeline = PipelineParams(parser)\n",
    "op = OptimizationParams(parser)\n",
    "\n",
    "# parser.add_argument('--ip', type=str, default=\"127.0.0.1\")\n",
    "# parser.add_argument('--port', type=int, default=6009)\n",
    "parser.add_argument('--debug_from', type=int, default=-1)\n",
    "# parser.add_argument('--detect_anomaly', action='store_true', default=False)\n",
    "parser.add_argument(\"--save_iterations\", nargs=\"+\", type=int, default=[100, 300, 500])\n",
    "parser.add_argument(\"--test_iterations\", nargs=\"+\", type=int, default=[100, 300, 500])\n",
    "parser.add_argument(\"--quiet\", action=\"store_true\")\n",
    "parser.add_argument(\"--checkpoint_iterations\", nargs=\"+\", type=int, default=[100,300, 500])\n",
    "parser.add_argument(\"--start_checkpoint\", type=str, default = None)\n",
    "\n",
    "args = parser.parse_args(sys.argv[1:])\n",
    "\n",
    "# Initialize system state (RNG)\n",
    "# safe_state(args.quiet)\n",
    "\n",
    "dataset, iteration, pipeline = model.extract(args), op.extract(args), pipeline.extract(args)\n",
    "\n",
    "testing_iterations, saving_iterations, checkpoint_iterations, checkpoint, debug_from = args.test_iterations, args.save_iterations, args.checkpoint_iterations, args.start_checkpoint, args.debug_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RGBD Scene\n",
      "Loading Training Cameras\n",
      "Loading Test Cameras\n",
      "Number of points at initialisation :  2073600\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize Gaussians with Zero Tensor\n",
    "    gaussians = GaussianModel(dataset.sh_degree)\n",
    "    # Load Actual Gaussians, Camera from PCD\n",
    "    scene = Scene(dataset, gaussians, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the viewpoint stack\n",
    "viewpoint_stack = None\n",
    "opt = op.extract(args)\n",
    "if not viewpoint_stack:\n",
    "    viewpoint_stack = scene.getTrainCameras().copy()\n",
    "    \n",
    "# viewpoint_cam = viewpoint_stack.pop(randint(0, len(viewpoint_stack)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveCurrentRender(viewpoint_stack[2], gaussians, pipeline, scene.model_path, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redner the first viewpoint\n",
    "renderGSwithStats(viewpoint_stack[0], gaussians, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_depth_map, visibility_map = getGaussianDepthMap(viewpoint_stack[10], gaussians, pipeline)\n",
    "\n",
    "# Account for 0 depth\n",
    "default_depth = 999.0\n",
    "raster_depth_map = raster_depth_map #* ~(raster_depth_map==default_depth)\n",
    "# print(raster_depth_map.min(), raster_depth_map.max())\n",
    "# showTensorDepthImage(raster_depth_map)\n",
    "\n",
    "image_array = raster_depth_map.cpu().numpy()\n",
    "plt.imshow(image_array, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array = visibility_map.cpu().numpy()\n",
    "plt.imshow(image_array < 0.5, cmap='Greys')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the depth map\n",
    "# Default_Depth = 1000000 # raster_depth_map.max()\n",
    "# mask = raster_depth_map == Default_Depth\n",
    "# # Filter out the default depth\n",
    "# raster_depth_map[mask] = 0\n",
    "# # Visualize the depth map\n",
    "# showTensorDepthImage(raster_depth_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ply_path = \"G:\\\\Universitat Siegen\\\\SA\\\\P-GPU\\\\Code\\\\gaussian-splatting\\\\data\\\\RGBD_Data\\\\SyntheticV1\\\\ply\"\n",
    "\n",
    "ply_files = os.listdir(ply_path)\n",
    "\n",
    "# 2D Rasterization\n",
    "def rasterizePoints_Perspective(points, colors,  viewpoint_cam, sub_pixel_level=1):\n",
    "       # Get Projection Matrix\n",
    "       P = viewpoint_cam.full_proj_transform\n",
    "\n",
    "       # Project points using projection matrix P in torch cuda and get result in NDC\n",
    "       points_homogeneous = torch.cat((points, torch.ones(points.shape[0], 1, device=points.device)), dim=1)\n",
    "       \n",
    "       projected_points_homogeneous = torch.matmul(points_homogeneous, P.t()) # Clip space coordinates\n",
    "\n",
    "\n",
    "       print(points_homogeneous.shape, projected_points_homogeneous.shape)\n",
    "       # print('Clip Space: ', projected_points_homogeneous.shape, projected_points_homogeneous.min(), projected_points_homogeneous.max())\n",
    "\n",
    "       # # Filter points outside clip space\n",
    "       # mask = (projected_points_homogeneous[:, 0] >= -1) & (projected_points_homogeneous[:, 0] < 1) & \\\n",
    "       #        (projected_points_homogeneous[:, 1] >= -1) & (projected_points_homogeneous[:, 1] < 1 ) # & \\\n",
    "              # (projected_points_homogeneous[:, 2] >= -1) & (projected_points_homogeneous[:, 2] < 1) // Removes points closer to camera so not included\n",
    "       # projected_points_homogeneous = projected_points_homogeneous[mask]\n",
    "\n",
    "       # Clip Space / Homogenous to NDC\n",
    "       assert projected_points_homogeneous.shape[1] == 4\n",
    "       # Extract x, y, z, w from the tensor\n",
    "       x, y, z, w = projected_points_homogeneous[:, 0], projected_points_homogeneous[:, 1], projected_points_homogeneous[:, 2], projected_points_homogeneous[:, 3]\n",
    "       x_ndc = x / w\n",
    "       y_ndc = y / w\n",
    "       z_ndc = z / w\n",
    "       projected_points_NDC = torch.stack((x_ndc, y_ndc, z_ndc), dim=1)\n",
    "\n",
    "       # print('NDC: ', projected_points_NDC.shape, projected_points_NDC.min(), projected_points_NDC.max())\n",
    "\n",
    "       # NDC to Image Space [-1,1] to [0,1]\n",
    "       projected_points_IS = (projected_points_NDC + 1) / 2\n",
    "\n",
    "       # Filter points outside image space\n",
    "       mask = (projected_points_IS[:, 0] >= 0) & (projected_points_IS[:, 0] < 1) & \\\n",
    "              (projected_points_IS[:, 1] >= 0) & (projected_points_IS[:, 1] < 1 ) # & \\\n",
    "              # (projected_points_IS[:, 2] >= 0) & (projected_points_IS[:, 2] < 1)\n",
    "       # projected_points_IS = projected_points_IS[mask]\n",
    "       # points_filtered = points[mask]\n",
    "       # colors_filtered = colors[mask]\n",
    "\n",
    "       # print(projected_points_IS.shape, projected_points_IS.min(), projected_points_IS.max())\n",
    "\n",
    "       # Get image width and height\n",
    "       img_width, img_height = viewpoint_cam.image_width, viewpoint_cam.image_height\n",
    "       # Calculate effective pixel width and height based on the sub-pixel variable\n",
    "       effective_pixel_width, effective_pixel_height = img_width // sub_pixel_level, img_height // sub_pixel_level\n",
    "       # Get pixel coordinates\n",
    "       pixel_coordinates = projected_points_IS[:,:2] * torch.tensor([effective_pixel_width, effective_pixel_height], device=points.device).float()\n",
    "       pixel_indices = torch.floor(pixel_coordinates).to(torch.int)\n",
    "\n",
    "\n",
    "       # Create an empty image for visualization\n",
    "       img = torch.zeros((968, 1296, 3), dtype=torch.uint8, device=points.device)\n",
    "       pixel_indices_long = pixel_indices.to(torch.long)\n",
    "       pixel_indices_long = torch.clamp(pixel_indices_long, 0, img_width - 1)\n",
    "       pixel_indices_long = torch.cat((pixel_indices_long[:, 1:], pixel_indices_long[:, :1]), dim=1)  # Swap x and y for indexing\n",
    "       colors_uint8 = (colors * 255).to(torch.uint8)\n",
    "       img[pixel_indices_long[:, 1], pixel_indices_long[:, 0]] = colors_uint8\n",
    "       img_cpu = img.cpu().detach().numpy()\n",
    "       plt.imshow(img_cpu.astype(int))  # Convert the image to int for display\n",
    "       plt.show()\n",
    "\n",
    "       # print(pixel_indices.shape, pixel_indices.min(), pixel_indices.max())\n",
    "       # return points_filtered,colors_filtered,pixel_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRGBDDepthMap(viewpoint_cam, points):\n",
    "    intrinsics = viewpoint_cam.intrinsics\n",
    "    c_x = intrinsics[0,2]\n",
    "    c_y = intrinsics[1,2]\n",
    "    f_x = intrinsics[0,0]\n",
    "    f_y = intrinsics[1,1]\n",
    "    W2C = viewpoint_cam.world_view_transform\n",
    "\n",
    "    img_width, img_height = viewpoint_cam.image_width, viewpoint_cam.image_height\n",
    "\n",
    "    full_proj_transform = viewpoint_cam.full_proj_transform\n",
    "\n",
    "    # print('W2C:',W2C.transpose(0,1))\n",
    "    # print('FP:',full_proj_transform.transpose(0,1))\n",
    "\n",
    "    points_homogeneous = torch.cat((points, torch.ones(points.shape[0], 1, device=points.device)), dim=1)\n",
    "    \n",
    "    points_camera_homogeneous = torch.matmul(points_homogeneous, W2C) # Clip space coordinates\n",
    "    Z = points_camera_homogeneous[:, 2]\n",
    "    \n",
    "    projected_points_homogeneous = torch.matmul(points_homogeneous, full_proj_transform)\n",
    "\n",
    "    assert projected_points_homogeneous.shape[1] == 4\n",
    "    # Extract x, y, z, w from the tensor\n",
    "    x, y, z, w = projected_points_homogeneous[:, 0], projected_points_homogeneous[:, 1], projected_points_homogeneous[:, 2], projected_points_homogeneous[:, 3]\n",
    "    x_ndc = x / w\n",
    "    y_ndc = y / w\n",
    "    z_ndc = z / w\n",
    "    projected_points_NDC = torch.stack((x_ndc, y_ndc, z_ndc), dim=1)\n",
    "    projected_points_IS = (projected_points_NDC + 1) / 2\n",
    "\n",
    "    # print('NDC: ', projected_points_NDC.shape, projected_points_NDC.min(), projected_points_NDC.max())\n",
    "    # print('IS: ', projected_points_IS.shape, projected_points_IS.min(), projected_points_IS.max())\n",
    "\n",
    "    u = projected_points_IS[:,0] * img_width\n",
    "    v = projected_points_IS[:,1] * img_height\n",
    "\n",
    "    # print('UV: ', u.min(), u.max(), v.min(), v.max())\n",
    "\n",
    "    # Filter points outside image space\n",
    "    image_space_mask = (u >= 0) & (u < img_width) & (v >= 0) & (v < img_height )\n",
    "    u = u[image_space_mask]\n",
    "    v = v[image_space_mask]\n",
    "    depth_filtered = Z[image_space_mask]\n",
    "\n",
    "    try:\n",
    "        raster_depth_map = torch.zeros((img_height, img_width), dtype=Z.dtype, device=points.device)\n",
    "\n",
    "        # Create Indices\n",
    "        u_long = u.to(torch.long)\n",
    "        v_long = v.to(torch.long)\n",
    "\n",
    "        # print(u_long, u_long.min(), u_long.max())\n",
    "\n",
    "        # Store Point Positions\n",
    "        raster_depth_map[v_long, u_long] = depth_filtered\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    return raster_depth_map\n",
    "\n",
    "def rasterizePoints(viewpoint_cam, points, colors=None, sub_pixel_level=1):\n",
    "    intrinsics = viewpoint_cam.intrinsics\n",
    "    c_x = intrinsics[0,2]\n",
    "    c_y = intrinsics[1,2]\n",
    "    f_x = intrinsics[0,0]\n",
    "    f_y = intrinsics[1,1]\n",
    "    W2C = viewpoint_cam.world_view_transform\n",
    "    full_proj_transform = viewpoint_cam.full_proj_transform\n",
    "    img_width, img_height = viewpoint_cam.image_width, viewpoint_cam.image_height\n",
    "\n",
    "    # print('W2C:',W2C.transpose(0,1))\n",
    "    # print('FP:',full_proj_transform.transpose(0,1))\n",
    "\n",
    "    # Project points using projection matrix P in torch cuda and get result in NDC\n",
    "    points_homogeneous = torch.cat((points, torch.ones(points.shape[0], 1, device=points.device)), dim=1)\n",
    "    # print(points_homogeneous.shape, colors.shape, W2C)\n",
    "    \n",
    "    points_camera_homogeneous = torch.matmul(points_homogeneous, W2C) # Clip space coordinates\n",
    "    projected_points_homogeneous = torch.matmul(points_homogeneous, full_proj_transform)\n",
    "\n",
    "    assert projected_points_homogeneous.shape[1] == 4\n",
    "    # Extract x, y, z, w from the tensor\n",
    "    x, y, z, w = projected_points_homogeneous[:, 0], projected_points_homogeneous[:, 1], projected_points_homogeneous[:, 2], projected_points_homogeneous[:, 3]\n",
    "    x_ndc = x / w\n",
    "    y_ndc = y / w\n",
    "    z_ndc = z / w\n",
    "    projected_points_NDC = torch.stack((x_ndc, y_ndc, z_ndc), dim=1)\n",
    "    projected_points_IS = (projected_points_NDC + 1) / 2\n",
    "\n",
    "    u = projected_points_IS[:,0] * img_width\n",
    "    v = projected_points_IS[:,1] * img_height\n",
    "    Z = points_camera_homogeneous[:, 2]\n",
    "\n",
    "    u = u.to(torch.int16)\n",
    "    v = v.to(torch.int16)\n",
    "\n",
    "    # Filter points outside image space\n",
    "    image_space_mask = (u >= 0) & (u < img_width) & (v >= 0) & (v < img_height )\n",
    "    u = u[image_space_mask]\n",
    "    v = v[image_space_mask]\n",
    "    points_filtered = points[image_space_mask]\n",
    "    depth_filtered = Z[image_space_mask]\n",
    "    # raster = showRasterizedImage(u,v, colors_filtered, img_height, img_width)\n",
    "\n",
    "    try:\n",
    "        # print(u.shape, v.shape, colors_filtered.shape)\n",
    "        raster_color = torch.zeros((img_height, img_width, 3), dtype=torch.uint8 , device=points.device)\n",
    "        raster_depth_map = torch.zeros((img_height, img_width), dtype=Z.dtype, device=points.device)\n",
    "\n",
    "        IS_point_positions = torch.zeros((img_height, img_width, 3), dtype=torch.float16 , device=points.device)\n",
    "        IS_point_colors = torch.zeros((img_height, img_width, 3), dtype=torch.float16 , device=points.device)\n",
    "\n",
    "        # Create Indices\n",
    "        u_long = u.to(torch.long)\n",
    "        v_long = v.to(torch.long)\n",
    "        # Store Point Positions\n",
    "        raster_depth_map[v_long, u_long] = depth_filtered\n",
    "        IS_point_positions[v_long, u_long] = points_filtered.to(torch.float16)\n",
    "        \n",
    "        # Store Point Colors\n",
    "        if colors is not None:\n",
    "            colors_filtered = colors[image_space_mask]\n",
    "            raster_color[v_long, u_long] = (colors_filtered * 255).to(torch.uint8)\n",
    "            IS_point_colors[v_long, u_long] = colors_filtered.to(torch.float16)\n",
    "        else:\n",
    "            colors_filtered = None\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    # pixel_indices_long = torch.clamp(pixel_indices_long, 0, img_width - 1)\n",
    "    # pixel_indices_long = torch.cat((pixel_indices_long[:, 1:], pixel_indices_long[:, :1]), dim=1)  # Swap x and y for indexing\n",
    "    # colors_uint8 = (colors * 255).to(torch.uint8)\n",
    "    # img[pixel_indices_long[:, 1], pixel_indices_long[:, 0]] = colors_uint8\n",
    "\n",
    "    pixel_indices = torch.stack((u,v), dim=1)\n",
    "\n",
    "    # print(pixel_indices.shape)\n",
    "\n",
    "    # # print(pixel_indices.shape, pixel_indices.min(), pixel_indices.max())\n",
    "    return points_filtered,colors_filtered,raster_depth_map, pixel_indices, raster_color, IS_point_positions, IS_point_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_points, pcd_colors = loadPLY(os.path.join(ply_path, ply_files[0]))\n",
    "depth_map = getRGBDDepthMap(viewpoint_stack[0], pcd_points)\n",
    "showTensorDepthImage(depth_map)\n",
    "\n",
    "# pcd_points, pcd_colors = loadPLY(os.path.join(ply_path, ply_files[0]))\n",
    "# filtered_points, filtered_colors, current_depth_map, indices, current_raster, raster_point_positions, raster_point_colors = rasterizePoints(viewpoint_stack[0], pcd_points, pcd_colors)\n",
    "# showTensorDepthImage(current_depth_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showTensorDepthImage(depth_map - raster_depth_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new points from PLY\n",
    "sub_pixel_level = 1\n",
    "\n",
    "global_points = None\n",
    "global_colors = None\n",
    "\n",
    "# Default Depth from GS\n",
    "default_depth = 1000000.0\n",
    "visibility_threshold = 0.5\n",
    "\n",
    "# for i in tqdm(range(len(viewpoint_stack))):\n",
    "for i in tqdm(range(0)):\n",
    "    viewpoint_cam = viewpoint_stack[i]\n",
    "    # Check if PLY file exists\n",
    "    ply_file_name = viewpoint_cam.colmap_id + \".ply\"\n",
    "    filtered_ply_file_name = viewpoint_cam.colmap_id + \"_filtered.ply\"\n",
    "\n",
    "    # print(ply_file_name)\n",
    "    # if ply_file_name not in ply_files:\n",
    "    #     continue\n",
    "\n",
    "    ply_file_path = os.path.join(ply_path, ply_file_name)\n",
    "    \n",
    "    if os.path.exists(ply_file_path) == False:\n",
    "        continue\n",
    "\n",
    "    print(ply_file_path)\n",
    "\n",
    "    pcd_points, pcd_colors = loadPLY(ply_file_path)\n",
    "    filtered_points, filtered_colors, current_depth_map, indices, current_raster, raster_point_positions, raster_point_colors = rasterizePoints(viewpoint_cam, pcd_points, pcd_colors)\n",
    "\n",
    "    # showTensorImage(current_raster)\n",
    "    # print('Indices:', indices.shape, indices.min(), indices.max())\n",
    "\n",
    "    # Load Initial Points\n",
    "    if global_points is None:\n",
    "        print(\"Initialized with points from first Frame\")\n",
    "        global_points = filtered_points\n",
    "        global_colors = filtered_colors\n",
    "    \n",
    "    if i > 0:\n",
    "        gaussian_points = scene.gaussians.get_xyz\n",
    "        global_filtered_points, global_filtered_colors, global_depth_map , global_pixel_indices, global_raster, _ , _ = rasterizePoints(viewpoint_cam, gaussian_points)\n",
    "\n",
    "        # global_depth_map, global_visibility_map = getGaussianDepthMap(viewpoint_cam, gaussians, pipeline)\n",
    "        # Identify Free Pixels\n",
    "        free_pixel_mask = (global_depth_map == 0) #(global_depth_map == default_depth) # (global_depth_map == 0) | \n",
    "        # free_pixel_mask = (global_visibility_map < visibility_threshold)\n",
    "        \n",
    "        # Identify Depth Pixels\n",
    "        # closer_depth_mask = (current_depth_map > 0) & (current_depth_map < global_depth_map) & (torch.abs(global_depth_map - current_depth_map) > 0.1)\n",
    "        mask = free_pixel_mask.int() # | closer_depth_mask\n",
    "        # showTensorDepthImage(mask)\n",
    "        # showTensorDepthImage(depth_near_mask.int())\n",
    "        # showTensorDepthImage(mask.int())\n",
    "        # showTensorImage(global_raster)\n",
    "        # showTensorImage(current_raster)\n",
    "\n",
    "        # Apply Mask to point positions that are black\n",
    "        raster_point_positions_masked = raster_point_positions.view(-1, 3) * mask.view(-1, 1)\n",
    "        current_raster_masked = raster_point_colors.view(-1, 3) * mask.view(-1, 1)\n",
    "\n",
    "        # showTensorImage(current_raster_masked.view(current_raster.shape)*255)\n",
    "\n",
    "        # # Find points which have all 0 values\n",
    "        invalid_points_mask = torch.all(raster_point_positions_masked == 0, dim=1, keepdim=True)\n",
    "        raster_point_positions_filtered = raster_point_positions_masked[~invalid_points_mask.view(-1)]\n",
    "        current_raster_filtered = current_raster_masked[~invalid_points_mask.view(-1)]\n",
    "        \n",
    "        \n",
    "        print(current_raster_filtered.shape, raster_point_positions_filtered.shape)\n",
    "\n",
    "        if raster_point_positions_filtered.shape[0] == 0:\n",
    "            continue\n",
    "        # saveTensorAsPLY(raster_point_positions_filtered,current_raster_filtered, 'G'+filtered_ply_file_name)\n",
    "        # saveTensorAsPLY(raster_point_positions_filtered,current_raster_filtered, filtered_ply_file_name)\n",
    "\n",
    "        # print(\"Global Points:\", global_points.shape, global_colors.shape)           \n",
    "        # Append to global points\n",
    "        global_points = torch.cat((global_points, raster_point_positions_filtered), dim=0)\n",
    "        global_colors = torch.cat((global_colors, current_raster_filtered), dim=0)\n",
    "        # print(\"Global Points Extended:\", global_points.shape, global_colors.shape)\n",
    "    \n",
    "        # Initialize 3D Gaussians with new points and add to scene\n",
    "        # scene.extendGaussians(BasicPointCloud(points=raster_point_positions_filtered.cpu().numpy(), colors=current_raster_filtered.cpu().numpy(), normals=None))\n",
    "\n",
    "# saveTensorAsPLY(global_points,global_colors, \"Global.ply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize Global Mask\n",
    "# global_black_mask = torch.all(global_raster == 0, dim=2, keepdim=True)\n",
    "# # Apply Mask to point positions that are black\n",
    "# raster_point_positions_masked = raster_point_positions.view(-1, 3) * global_black_mask.view(-1, 1)\n",
    "# current_raster_masked = current_raster.view(-1, 3) * global_black_mask.view(-1, 1)\n",
    "\n",
    "# # Find points which have all 0 values\n",
    "# invalid_points_mask = torch.all(raster_point_positions_masked == 0, dim=1, keepdim=True)\n",
    "\n",
    "# raster_point_positions_filtered = raster_point_positions_masked[~invalid_points_mask.view(-1)]\n",
    "# current_raster_filtered = current_raster_masked[~invalid_points_mask.view(-1)]\n",
    "\n",
    "# print(raster_point_positions_masked.shape, current_raster_masked.shape)\n",
    "# print(raster_point_positions_filtered.shape, current_raster_filtered.shape)\n",
    "\n",
    "# showTensorImage(global_raster)\n",
    "# showTensorImage(current_raster)\n",
    "# showTensorImage(current_raster_masked.view(current_raster.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(viewpoint_stack), len(ply_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progressive Fusion\n",
    "first_viewpoint = 0\n",
    "tb_writer = prepare_output_and_logger(dataset)\n",
    "# gaussians = GaussianModel(dataset.sh_degree)\n",
    "# scene = Scene(dataset, gaussians)\n",
    "gaussians.training_setup(opt)\n",
    "# if checkpoint:\n",
    "#     (model_params, first_iter) = torch.load(checkpoint)\n",
    "#     gaussians.restore(model_params, opt)\n",
    "\n",
    "bg_color = [1, 1, 1] if dataset.white_background else [0, 0, 0]\n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "iter_start = torch.cuda.Event(enable_timing = True)\n",
    "iter_end = torch.cuda.Event(enable_timing = True)\n",
    "\n",
    "batch_iter = 10\n",
    "\n",
    "viewpoint_stack = scene.getTrainCameras().copy()\n",
    "num_viewpoints = len(viewpoint_stack)\n",
    "\n",
    "ema_loss_for_log = 0.0\n",
    "progress_bar = tqdm(range(first_viewpoint, num_viewpoints), desc=\"Training progress\")\n",
    "first_viewpoint += 1\n",
    "\n",
    "# Scene is Already loaded with First Frame\n",
    "frame_batch_size = 5\n",
    "frame_step = 3\n",
    "visibility_threshold = 0.5\n",
    "assert viewpoint_stack != None, \"Viewpoint Stack is None\"\n",
    "\n",
    "for viewpoint_index in range(first_viewpoint, num_viewpoints, frame_step):\n",
    "    # Fuse K frames into Global Field\n",
    "    # Gaussian Densification / Fusion\n",
    "    start_cam_index = viewpoint_index\n",
    "    end_cam_index = min(num_viewpoints, viewpoint_index + frame_batch_size)\n",
    "\n",
    "    # print(\"Fusing Frames: \", list(range(start_cam_index, end_cam_index)))\n",
    "    for K_i in range(start_cam_index, end_cam_index):\n",
    "        viewpoint_cam = viewpoint_stack[K_i]\n",
    "        ply_file_name = viewpoint_cam.colmap_id + \".ply\"        \n",
    "\n",
    "        ply_file_path = os.path.join(ply_path, ply_file_name)\n",
    "        assert os.path.exists(ply_file_path), \"PLY File does not exist\"\n",
    "\n",
    "        pcd_points, pcd_colors = loadPLY(ply_file_path)\n",
    "        filtered_points, filtered_colors, current_depth_map, indices, current_raster, raster_point_positions, raster_point_colors = rasterizePoints(viewpoint_cam, pcd_points, pcd_colors)\n",
    "        global_depth_map, global_visibility_map = getGaussianDepthMap(viewpoint_cam, gaussians, pipeline)\n",
    "        \n",
    "        # Identify Free Pixels\n",
    "        free_pixel_mask = (global_visibility_map < visibility_threshold)\n",
    "        mask = free_pixel_mask.int()\n",
    "\n",
    "        # Apply Mask to point positions\n",
    "        raster_point_positions_masked = raster_point_positions.view(-1, 3) * mask.view(-1, 1)\n",
    "        current_raster_masked = raster_point_colors.view(-1, 3) * mask.view(-1, 1)\n",
    "\n",
    "        # showTensorImage(current_raster_masked.view(current_raster.shape)*255)\n",
    "\n",
    "        # Filter points which are at origin\n",
    "        invalid_points_mask = torch.all(raster_point_positions_masked == 0, dim=1, keepdim=True)\n",
    "        raster_point_positions_filtered = raster_point_positions_masked[~invalid_points_mask.view(-1)]\n",
    "        current_raster_filtered = current_raster_masked[~invalid_points_mask.view(-1)]\n",
    "        \n",
    "\n",
    "        # Fuse points into global field\n",
    "        # Initialize 3D Gaussians with new points and add to scene\n",
    "        scene.extendGaussians(BasicPointCloud(points=raster_point_positions_filtered.cpu().numpy(), colors=current_raster_filtered.cpu().numpy(), normals=None))\n",
    "\n",
    "\n",
    "    # Clear Memory\n",
    "    current_raster_filtered = None\n",
    "    raster_point_positions_filtered = None\n",
    "    raster_point_positions_masked = None\n",
    "    current_raster_masked = None\n",
    "    raster_point_positions = None\n",
    "    raster_point_colors = None\n",
    "    current_raster = None\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "    # Optimize over K frames\n",
    "    # Gaussian Optimization\n",
    "    for iter in range(batch_iter):\n",
    "        for cam_index in range(start_cam_index, end_cam_index):\n",
    "            # Pick the current Camera\n",
    "            viewpoint_cam = viewpoint_stack[cam_index]\n",
    "            \n",
    "            # Render\n",
    "            bg = torch.rand((3), device=\"cuda\") if opt.random_background else background\n",
    "\n",
    "            render_pkg = render(viewpoint_cam, gaussians, pipeline, bg)\n",
    "            image, viewspace_point_tensor, visibility_filter, radii = render_pkg[\"render\"], render_pkg[\"viewspace_points\"], render_pkg[\"visibility_filter\"], render_pkg[\"radii\"]\n",
    "\n",
    "            # Loss\n",
    "            gt_image = viewpoint_cam.original_image.cuda()\n",
    "            Ll1 = l1_loss(image, gt_image)\n",
    "            loss = (1.0 - opt.lambda_dssim) * Ll1 + opt.lambda_dssim * (1.0 - ssim(image, gt_image))\n",
    "            loss.backward()\n",
    "\n",
    "        # Clear Memory\n",
    "        gt_image = None\n",
    "        image = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Progress bar\n",
    "            ema_loss_for_log = 0.4 * loss.item() + 0.6 * ema_loss_for_log\n",
    "            progress_bar.set_postfix({\"Loss\": f\"{ema_loss_for_log:.{7}f}\"})\n",
    "    \n",
    "            gaussians.optimizer.step()\n",
    "            gaussians.optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "    progress_bar.update(frame_step)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tuning\n",
    "first_iter = 0\n",
    "tb_writer = prepare_output_and_logger(dataset)\n",
    "# gaussians = GaussianModel(dataset.sh_degree)\n",
    "# scene = Scene(dataset, gaussians)\n",
    "gaussians.training_setup(opt)\n",
    "# if checkpoint:\n",
    "#     (model_params, first_iter) = torch.load(checkpoint)\n",
    "#     gaussians.restore(model_params, opt)\n",
    "\n",
    "bg_color = [1, 1, 1] if dataset.white_background else [0, 0, 0]\n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "iter_start = torch.cuda.Event(enable_timing = True)\n",
    "iter_end = torch.cuda.Event(enable_timing = True)\n",
    "\n",
    "viewpoint_stack = None\n",
    "ema_loss_for_log = 0.0\n",
    "progress_bar = tqdm(range(first_iter, opt.iterations), desc=\"Training progress\")\n",
    "first_iter += 1\n",
    "\n",
    "# Optimize over N frames\n",
    "for iteration in range(first_iter, opt.iterations + 1):        \n",
    "    if network_gui.conn == None:\n",
    "        network_gui.try_connect()\n",
    "    while network_gui.conn != None:\n",
    "        try:\n",
    "            net_image_bytes = None\n",
    "            custom_cam, do_training, pipeline.convert_SHs_python, pipeline.compute_cov3D_python, keep_alive, scaling_modifer = network_gui.receive()\n",
    "            if custom_cam != None:\n",
    "                net_image = render(custom_cam, gaussians, pipeline, background, scaling_modifer)[\"render\"]\n",
    "                net_image_bytes = memoryview((torch.clamp(net_image, min=0, max=1.0) * 255).byte().permute(1, 2, 0).contiguous().cpu().numpy())\n",
    "            network_gui.send(net_image_bytes, dataset.source_path)\n",
    "            if do_training and ((iteration < int(opt.iterations)) or not keep_alive):\n",
    "                break\n",
    "        except Exception as e:\n",
    "            network_gui.conn = None\n",
    "\n",
    "    iter_start.record()\n",
    "\n",
    "    gaussians.update_learning_rate(iteration)\n",
    "\n",
    "    # Every 1000 its we increase the levels of SH up to a maximum degree\n",
    "    if iteration % 1000 == 0:\n",
    "        gaussians.oneupSHdegree()\n",
    "\n",
    "    # Pick a random Camera\n",
    "    if not viewpoint_stack:\n",
    "        viewpoint_stack = scene.getTrainCameras().copy()\n",
    "    viewpoint_cam = viewpoint_stack.pop(randint(0, len(viewpoint_stack)-1))\n",
    "\n",
    "    # Render\n",
    "    if (iteration - 1) == debug_from:\n",
    "        pipeline.debug = True\n",
    "\n",
    "    bg = torch.rand((3), device=\"cuda\") if opt.random_background else background\n",
    "\n",
    "    render_pkg = render(viewpoint_cam, gaussians, pipeline, bg)\n",
    "    image, viewspace_point_tensor, visibility_filter, radii = render_pkg[\"render\"], render_pkg[\"viewspace_points\"], render_pkg[\"visibility_filter\"], render_pkg[\"radii\"]\n",
    "\n",
    "    # print(visibility_filter.shape, radii.shape, viewspace_point_tensor.shape)\n",
    "\n",
    "    # Loss\n",
    "    gt_image = viewpoint_cam.original_image.cuda()\n",
    "    Ll1 = l1_loss(image, gt_image)\n",
    "    loss = (1.0 - opt.lambda_dssim) * Ll1 + opt.lambda_dssim * (1.0 - ssim(image, gt_image))\n",
    "    loss.backward()\n",
    "\n",
    "    iter_end.record()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Progress bar\n",
    "        ema_loss_for_log = 0.4 * loss.item() + 0.6 * ema_loss_for_log\n",
    "        if iteration % 10 == 0:\n",
    "            progress_bar.set_postfix({\"Loss\": f\"{ema_loss_for_log:.{7}f}\"})\n",
    "            progress_bar.update(10)\n",
    "        if iteration == opt.iterations:\n",
    "            progress_bar.close()\n",
    "\n",
    "        # Log and save\n",
    "        # training_report(tb_writer, iteration, Ll1, loss, l1_loss, iter_start.elapsed_time(iter_end), testing_iterations, scene, render, (pipeline, background))\n",
    "        if (iteration in saving_iterations):\n",
    "            print(\"\\n[ITER {}] Saving Gaussians\".format(iteration))\n",
    "            scene.save(iteration)\n",
    "\n",
    "        # Densification\n",
    "        if iteration < opt.densify_until_iter:\n",
    "            # Keep track of max radii in image-space for pruning\n",
    "            gaussians.max_radii2D[visibility_filter] = torch.max(gaussians.max_radii2D[visibility_filter], radii[visibility_filter])\n",
    "            # print(viewspace_point_tensor.shape, visibility_filter.shape)\n",
    "            gaussians.add_densification_stats(viewspace_point_tensor, visibility_filter)\n",
    "\n",
    "            if iteration > opt.densify_from_iter and iteration % opt.densification_interval == 0:\n",
    "                size_threshold = 20 if iteration > opt.opacity_reset_interval else None\n",
    "                gaussians.densify_and_prune(opt.densify_grad_threshold, 0.005, scene.cameras_extent, size_threshold)\n",
    "            \n",
    "            if iteration % opt.opacity_reset_interval == 0 or (dataset.white_background and iteration == opt.densify_from_iter):\n",
    "                gaussians.reset_opacity()\n",
    "\n",
    "        # Optimizer step\n",
    "        if iteration < opt.iterations:\n",
    "            gaussians.optimizer.step()\n",
    "            gaussians.optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "        if (iteration in checkpoint_iterations):\n",
    "            print(\"\\n[ITER {}] Saving Checkpoint\".format(iteration))\n",
    "            torch.save((gaussians.capture(), iteration), scene.model_path + \"/chkpnt\" + str(iteration) + \".pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [0,2]\n",
    "\n",
    "l.pop(0)\n",
    "l.pop(0)\n",
    "if not l:\n",
    "    print(\"Empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
