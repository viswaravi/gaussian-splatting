{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import os\n",
    "from os import makedirs\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from random import randint\n",
    "from scipy.spatial.transform import Rotation\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "import scenenet_pb2 as sn\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "    return v/np.linalg.norm(v)\n",
    "\n",
    "def camera_to_world_matrix(view_pose):\n",
    "    camera_position = position_to_np_array(view_pose.camera)\n",
    "    look_at = position_to_np_array(view_pose.lookat)\n",
    "    up = np.array([0, 1, 0])\n",
    "\n",
    "    # Calculate the forward, right, and up vectors\n",
    "    forward = normalize(look_at - camera_position)\n",
    "    right = normalize(np.cross(up, forward))\n",
    "    up = np.cross(forward, right)\n",
    "\n",
    "    # Create a 4x4 transformation matrix\n",
    "    world_to_camera_matrix = np.eye(4)\n",
    "    world_to_camera_matrix[:3, 0] = right\n",
    "    world_to_camera_matrix[:3, 1] = up\n",
    "    world_to_camera_matrix[:3, 2] = -forward\n",
    "    world_to_camera_matrix[:3, 3] = camera_position\n",
    "\n",
    "    return world_to_camera_matrix\n",
    "\n",
    "def world_to_camera_with_pose(view_pose):\n",
    "    lookat_pose = position_to_np_array(view_pose.lookat)\n",
    "    camera_pose = position_to_np_array(view_pose.camera)\n",
    "    up = np.array([0,1,0])\n",
    "    R = np.diag(np.ones(4))\n",
    "    R[2,:3] = normalize(lookat_pose - camera_pose)\n",
    "    R[0,:3] = normalize(np.cross(R[2,:3],up))\n",
    "    R[1,:3] = -normalize(np.cross(R[0,:3],R[2,:3]))\n",
    "    T = np.diag(np.ones(4))\n",
    "    T[:3,3] = -camera_pose\n",
    "    return R.dot(T)\n",
    "\n",
    "def camera_to_world_with_pose(view_pose):\n",
    "    return np.linalg.inv(world_to_camera_with_pose(view_pose))\n",
    "\n",
    "def camera_intrinsic_transform(vfov=45,hfov=60,pixel_width=320,pixel_height=240):\n",
    "    camera_intrinsics = np.zeros((3,4))\n",
    "    camera_intrinsics[2,2] = 1\n",
    "    camera_intrinsics[0,0] = (pixel_width/2.0)/math.tan(math.radians(hfov/2.0))\n",
    "    camera_intrinsics[0,2] = pixel_width/2.0\n",
    "    camera_intrinsics[1,1] = (pixel_height/2.0)/math.tan(math.radians(vfov/2.0))\n",
    "    camera_intrinsics[1,2] = pixel_height/2.0\n",
    "    return camera_intrinsics\n",
    "\n",
    "def position_to_np_array(position,homogenous=False):\n",
    "    if not homogenous:\n",
    "        return np.array([position.x,position.y,position.z])\n",
    "    return np.array([position.x,position.y,position.z,1.0])\n",
    "\n",
    "def interpolate_poses(start_pose,end_pose,alpha):\n",
    "    assert alpha >= 0.0\n",
    "    assert alpha <= 1.0\n",
    "    camera_pose = alpha * position_to_np_array(end_pose.camera)\n",
    "    camera_pose += (1.0 - alpha) * position_to_np_array(start_pose.camera)\n",
    "    lookat_pose = alpha * position_to_np_array(end_pose.lookat)\n",
    "    lookat_pose += (1.0 - alpha) * position_to_np_array(start_pose.lookat)\n",
    "    timestamp = alpha * end_pose.timestamp + (1.0 - alpha) * start_pose.timestamp\n",
    "    pose = sn.Pose()\n",
    "    pose.camera.x = camera_pose[0]\n",
    "    pose.camera.y = camera_pose[1]\n",
    "    pose.camera.z = camera_pose[2]\n",
    "    pose.lookat.x = lookat_pose[0]\n",
    "    pose.lookat.y = lookat_pose[1]\n",
    "    pose.lookat.z = lookat_pose[2]\n",
    "    pose.timestamp = timestamp\n",
    "    return pose\n",
    "\n",
    "def extract_intrinsics(intrinsic_matrix):\n",
    "    fx = intrinsic_matrix[0, 0]\n",
    "    fy = intrinsic_matrix[1, 1]\n",
    "    cx = intrinsic_matrix[0, 2]\n",
    "    cy = intrinsic_matrix[1, 2] \n",
    "\n",
    "    return cx, cy, fx, fy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "G:\\Universitat Siegen\\SA\\P-GPU\\Code\\gaussian-splatting\\data\\SceneNet\\train\\784\n",
      "G:\\Universitat Siegen\\SA\\P-GPU\\Code\\gaussian-splatting\\data\\SceneNet\\train\\784\\photo\n",
      "G:\\Universitat Siegen\\SA\\P-GPU\\Code\\gaussian-splatting\\data\\SceneNet\\train\\784\\depth\n"
     ]
    }
   ],
   "source": [
    "scene_path = \"G:\\\\Universitat Siegen\\\\SA\\\\P-GPU\\\\Code\\\\gaussian-splatting\\\\data\\\\SceneNet\\\\train\"\n",
    "protobuf_path = os.path.join(scene_path, \"scenenet_rgbd_train_0.pb\")\n",
    "\n",
    "# Read the protobuf file\n",
    "trajectories = sn.Trajectories()\n",
    "try:\n",
    "    with open(protobuf_path,'rb') as f:\n",
    "        trajectories.ParseFromString(f.read())\n",
    "except IOError:\n",
    "    print('Scenenet protobuf data not found at location:{0}'.format(scene_path))\n",
    "    print('Please ensure you have copied the pb file to the data directory')\n",
    "\n",
    "# Extracting the camera intrinsics\n",
    "intrinsic_matrix = camera_intrinsic_transform()\n",
    "image_width = 320\n",
    "image_height = 240\n",
    "FovX = 60\n",
    "FovY = 45\n",
    "\n",
    "# Select scene trajectory\n",
    "traj = trajectories.trajectories[1]\n",
    "scene_id = traj.render_path.split('/')[-1]\n",
    "\n",
    "scene_path = os.path.join(scene_path, scene_id)\n",
    "images_path = os.path.join(scene_path, \"photo\")\n",
    "depths_path = os.path.join(scene_path, \"depth\")\n",
    "ply_path = os.path.join(scene_path, \"ply\")\n",
    "\n",
    "print(scene_id)\n",
    "print(scene_path)\n",
    "print(images_path)\n",
    "print(depths_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPCD(points, colors=None):\n",
    "    # Extract the transformed X, Y, Z coordinates\n",
    "    X = points[0, :]\n",
    "    Y = points[1, :]\n",
    "    Z = points[2, :]\n",
    "    # Stack the global coordinates\n",
    "    point_cloud_points = np.vstack((X, Y, Z)).T\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(point_cloud_points)\n",
    "    # Add color to the point cloud\n",
    "    if colors is not None:\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    return pcd\n",
    "\n",
    "def getArrowMesh():\n",
    "    arrow = o3d.geometry.TriangleMesh.create_arrow(cylinder_radius=0.1,\n",
    "                                               cone_radius=0.20,\n",
    "                                               cylinder_height=0.25,\n",
    "                                               cone_height=0.1,\n",
    "                                               resolution=20,\n",
    "                                               cylinder_split=4,\n",
    "                                               cone_split=1)\n",
    "    arrow.compute_vertex_normals()\n",
    "    arrow.paint_uniform_color([1, 0, 0])\n",
    "    return arrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 300\n",
      "Start index: 0\n"
     ]
    }
   ],
   "source": [
    "# Get the list of files in the directory\n",
    "depth_files = os.listdir(depths_path)\n",
    "image_files = os.listdir(images_path)\n",
    "\n",
    "\n",
    "# Sort the files to process depth and color images together\n",
    "depth_files = sorted(depth_files, key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "start_index = 0\n",
    "ply_files = os.listdir(ply_path)\n",
    "if len(ply_files) > 0:\n",
    "    start_index = int(ply_files[-1].split('.')[0])\n",
    "\n",
    "# frame_step = 5\n",
    "# Loop through each pair of depth and color images\n",
    "# for i in tqdm(range(1)):\n",
    "\n",
    "print(\"Number of files:\", len(depth_files))\n",
    "print(\"Start index:\", start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_to_ray(pixel,vfov=45,hfov=60,pixel_width=320,pixel_height=240):\n",
    "    x, y = pixel\n",
    "    x_vect = math.tan(math.radians(hfov/2.0)) * ((2.0 * ((x+0.5)/pixel_width)) - 1.0)\n",
    "    y_vect = math.tan(math.radians(vfov/2.0)) * ((2.0 * ((y+0.5)/pixel_height)) - 1.0)\n",
    "    return (x_vect,y_vect,1.0)\n",
    "\n",
    "def normalised_pixel_to_ray_array(width=320,height=240):\n",
    "    pixel_to_ray_array = np.zeros((height,width,3))\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            pixel_to_ray_array[y,x] = normalize(np.array(pixel_to_ray((x,y),pixel_height=height,pixel_width=width)))\n",
    "    return pixel_to_ray_array\n",
    "\n",
    "def points_in_camera_coords(depth_map,pixel_to_ray_array):\n",
    "    assert depth_map.shape[0] == pixel_to_ray_array.shape[0]\n",
    "    assert depth_map.shape[1] == pixel_to_ray_array.shape[1]\n",
    "    assert len(depth_map.shape) == 2\n",
    "    assert pixel_to_ray_array.shape[2] == 3\n",
    "    camera_relative_xyz = np.ones((depth_map.shape[0],depth_map.shape[1],4))\n",
    "    for i in range(3):\n",
    "        camera_relative_xyz[:,:,i] = depth_map * pixel_to_ray_array[:,:,i]\n",
    "    return camera_relative_xyz\n",
    "\n",
    "def flatten_points(points):\n",
    "    return points.reshape(-1, 4)\n",
    "\n",
    "def reshape_points(height,width,points):\n",
    "    other_dim = points.shape[1]\n",
    "    return points.reshape(height,width,other_dim)\n",
    "\n",
    "def transform_points(transform,points):\n",
    "    assert points.shape[2] == 4\n",
    "    height = points.shape[0]\n",
    "    width = points.shape[1]\n",
    "    points = flatten_points(points)\n",
    "    return reshape_points(height,width,(transform.dot(points.T)).T)\n",
    "\n",
    "def reshape_for_pcd(points_img):\n",
    "    points_flat = points_img.reshape(-1,4)\n",
    "    x = points_flat[:,0]\n",
    "    y = points_flat[:,1]\n",
    "    z = points_flat[:,2]\n",
    "    w = points_flat[:,3]\n",
    "    points_reshaped = np.array([x,y,z,w])\n",
    "    return points_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth File: G:\\Universitat Siegen\\SA\\P-GPU\\Code\\gaussian-splatting\\data\\SceneNet\\train\\784\\depth\\0.png\n",
      "Depth: (240, 320) 1184 5725\n",
      "[[4.714 4.703 4.692 ... 1.573 1.558 1.544]\n",
      " [4.719 4.708 4.697 ... 1.57  1.555 1.54 ]\n",
      " [4.723 4.713 4.702 ... 1.566 1.551 1.537]\n",
      " ...\n",
      " [3.578 3.573 3.568 ... 1.201 1.193 1.184]\n",
      " [3.57  3.565 3.56  ... 1.201 1.193 1.184]\n",
      " [3.561 3.556 3.552 ... 1.201 1.193 1.184]]\n",
      "Camera: (240, 320, 4) (4, 76800)\n",
      "C2W: [[-0.97482455 -0.11887642  0.18864118 -3.76251507]\n",
      " [ 0.         -0.84602582 -0.53314192  2.6642251 ]\n",
      " [ 0.22297331 -0.51971983  0.82472674  4.68591022]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "World: (240, 320, 4) (4, 76800)\n",
      "[[[-0.6896541   1.95570435  8.18981088  1.        ]\n",
      "  [-0.7061083   1.95638093  8.18955737  1.        ]\n",
      "  [-0.72256539  1.95706419  8.18926482  1.        ]\n",
      "  ...\n",
      "  [-4.16974418  2.42714835  6.18667272  1.        ]\n",
      "  [-4.16978511  2.42973196  6.17134218  1.        ]\n",
      "  [-4.17000096  2.43215975  6.15697682  1.        ]]\n",
      "\n",
      " [[-0.68506753  1.94302466  8.189929    1.        ]\n",
      "  [-0.70153765  1.94370967  8.18969942  1.        ]\n",
      "  [-0.71801079  1.9444015   8.18943084  1.        ]\n",
      "  ...\n",
      "  [-4.16988174  2.42361981  6.18292776  1.        ]\n",
      "  [-4.16990833  2.42624688  6.16760591  1.        ]\n",
      "  [-4.16984587  2.42886831  6.15229653  1.        ]]\n",
      "\n",
      " [[-0.68114983  1.93045763  8.18926613  1.        ]\n",
      "  [-0.69698341  1.93099565  8.18980223  1.        ]\n",
      "  [-0.71347255  1.93169595  8.18955758  1.        ]\n",
      "  ...\n",
      "  [-4.16975461  2.42025644  6.17822093  1.        ]\n",
      "  [-4.1697643   2.4229268   6.1629086   1.        ]\n",
      "  [-4.16995     2.42543607  6.14856041  1.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.71027642  0.10061619  6.10658257  1.        ]\n",
      "  [-1.72059263  0.10064788  6.10891886  1.        ]\n",
      "  [-1.73092252  0.10069714  6.11124531  1.        ]\n",
      "  ...\n",
      "  [-4.16979191  1.80133354  5.41525564  1.        ]\n",
      "  [-4.16995665  1.80826439  5.41018338  1.        ]\n",
      "  [-4.16972823  1.8158984   5.40450277  1.        ]]\n",
      "\n",
      " [[-1.71798365  0.10023262  6.0968439   1.        ]\n",
      "  [-1.72827879  0.10027902  6.09916472  1.        ]\n",
      "  [-1.73858746  0.10034303  6.10147575  1.        ]\n",
      "  ...\n",
      "  [-4.16981119  1.799275    5.41280237  1.        ]\n",
      "  [-4.1699736   1.80622015  5.40775095  1.        ]\n",
      "  [-4.16974287  1.81387017  5.40209317  1.        ]]\n",
      "\n",
      " [[-1.72625725  0.1006259   6.08674364  1.        ]\n",
      "  [-1.7365292   0.1006881   6.08904788  1.        ]\n",
      "  [-1.74624686  0.10004601  6.09173813  1.        ]\n",
      "  ...\n",
      "  [-4.16982755  1.79722648  5.41034859  1.        ]\n",
      "  [-4.16998763  1.80418583  5.40531801  1.        ]\n",
      "  [-4.16975462  1.81185177  5.39968305  1.        ]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert Depth Images to Point Clouds\n",
    "# Initialize an empty point cloud\n",
    "point_cloud = o3d.geometry.PointCloud()\n",
    "Rpcd = None\n",
    "resolution_scale = 1\n",
    "arrows = o3d.geometry.TriangleMesh()\n",
    "\n",
    "# Intrinsic parameters of the camera (you may need to adjust these values)\n",
    "cx, cy, fx, fy = extract_intrinsics(intrinsic_matrix)\n",
    "\n",
    "cached_pixel_to_ray_array = normalised_pixel_to_ray_array()\n",
    "\n",
    "# Using Numpy to create point clouds\n",
    "# for i in tqdm(range(start_index, len(depth_files),frame_step)):\n",
    "# for i in range(start_index, len(files),frame_step):\n",
    "# for i in tqdm(range(0,6,2)):\n",
    "for i in tqdm(range(1)):\n",
    "    view = traj.views[i]\n",
    "    depth_file_name = depth_files[i]\n",
    "    image_file_name = image_files[i]\n",
    "\n",
    "    depth_file = os.path.join(depths_path, depth_file_name)\n",
    "    image_file = os.path.join(images_path, image_file_name)\n",
    "\n",
    "    print(\"Depth File:\", depth_file)\n",
    "\n",
    "    # Get the corresponding color image\n",
    "    color_file = os.path.join(image_file)\n",
    "    color_image = cv2.imread(color_file)\n",
    "    depth_image = cv2.imread(depth_file, cv2.IMREAD_ANYDEPTH)\n",
    "    \n",
    "    frame_id = view.frame_num\n",
    "\n",
    "    width, height = color_image.shape[1], color_image.shape[0]\n",
    "\n",
    "    # Get camera pose\n",
    "    # print(view)\n",
    "    ground_truth_pose = interpolate_poses(view.shutter_open,view.shutter_close,0.5)\n",
    "    W2C = world_to_camera_with_pose(ground_truth_pose)\n",
    "    C2W = camera_to_world_with_pose(ground_truth_pose)\n",
    "    pose = C2W\n",
    "\n",
    "    # Convert images to numpy arrays\n",
    "    depth_array = np.asarray(depth_image)\n",
    "    color_array = np.asarray(color_image)\n",
    "    color_array = color_array[:, :, :3]\n",
    "\n",
    "    print('Depth:', depth_array.shape, np.min(depth_array), np.max(depth_array))\n",
    "    print(depth_array * 0.001) \n",
    "\n",
    "    points_in_camera = points_in_camera_coords(depth_array * 0.001,cached_pixel_to_ray_array)\n",
    "    points_in_camera_reshaped = reshape_for_pcd(points_in_camera)\n",
    "    print('Camera:', points_in_camera.shape, points_in_camera_reshaped.shape)\n",
    "    # print(points_in_camera)\n",
    "\n",
    "    print('C2W:', C2W)\n",
    "\n",
    "\n",
    "    points_in_world = transform_points(C2W,points_in_camera)\n",
    "    points_in_world_reshaped = reshape_for_pcd(points_in_world)\n",
    "    print('World:', points_in_world.shape, points_in_world_reshaped.shape)\n",
    "    print(points_in_world)\n",
    "\n",
    "    # Convert depth to 3D coordinates in camera coordinates\n",
    "    # u, v = np.meshgrid(np.arange(width), np.arange(height-1, -1, -1))\n",
    "    u, v = np.meshgrid(np.arange(width), np.arange(height))\n",
    "    u = u.flatten()\n",
    "    v = v.flatten()\n",
    "\n",
    "    depth_scale = 1.0  / 1000\n",
    "    depth = depth_array.flatten() * depth_scale\n",
    "\n",
    "    # print(\"Color Array:\", color_array.shape)\n",
    "    # print(\"Depth Array:\", depth_array.shape,np.min(depth), np.max(depth))\n",
    "\n",
    "    X = ((u - cx) * depth / fx)\n",
    "    Y = ((v - cy) * depth / fy)\n",
    "    # Z = -depth\n",
    "    Z = depth\n",
    "\n",
    "    # plt.imshow(color_array)\n",
    "    # plt.imshow(depth_array)\n",
    "    # plt.colorbar()\n",
    "    # plt.show()\n",
    "\n",
    "    rgb_values = color_array.reshape((-1, 3)) / 255.0\n",
    "    # Stack the 3D coordinates\n",
    "    points_camera = np.vstack((X, Y, Z, np.ones_like(X)))\n",
    "    \n",
    "    n_pose = pose\n",
    "    # flip_YZ = np.array([[1, 0, 0, 0],[0, -1, 0, 0],[0, 0, -1, 0],[0, 0, 0, 1]])\n",
    "    # n_pose = np.dot(pose, flip_YZ)\n",
    "    # print(pose, n_pose)\n",
    "\n",
    "    # Transform to global coordinates\n",
    "    points_global = np.dot(n_pose, points_camera)\n",
    "    # points_global = np.dot( flip_YZ, points_global)\n",
    "\n",
    "#     # Transform points again to camera coordinates and check\n",
    "#     if False:\n",
    "#         # Extract the transformed X, Y, Z coordinates\n",
    "#         R = pose[:3, :3]\n",
    "#         t = pose[:3, 3]\n",
    "#         W2C = getWorld2View3(R, t)\n",
    "#         W2C = np.dot(flip_YZ, W2C)\n",
    "#         points_camera_again =  np.dot(W2C,points_global)\n",
    "#         pcd = getPCD(points_camera_again, rgb_values)\n",
    "#         # print('PCA:', np.shape(points_camera_again), points_camera_again[2, :])\n",
    "#         # Transform the points to OpenCV coordinate system\n",
    "#         # pcd.transform([[1, 0, 0, 0],[0, -1, 0, 0],[0, 0, -1, 0],[0, 0, 0, 1]])\n",
    "#         point_cloud += pcd\n",
    "\n",
    "    if False:\n",
    "        pcd = getPCD(points_in_camera_reshaped, rgb_values)\n",
    "        point_cloud += pcd\n",
    "\n",
    "    points_global = points_in_world_reshaped\n",
    "    # Extract the transformed X, Y, Z coordinates\n",
    "    X_global = points_global[0, :] / points_global[3 , :]\n",
    "    Y_global = points_global[1, :] / points_global[3 , :]\n",
    "    Z_global = points_global[2, :] / points_global[3 , :]\n",
    "    # Stack the global coordinates\n",
    "    point_cloud_global = np.vstack((X_global, Y_global, Z_global)).T\n",
    "\n",
    "    # Create an Open3D PointCloud\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    # Set the points in the PointCloud \n",
    "    pcd.points = o3d.utility.Vector3dVector(point_cloud_global)\n",
    "\n",
    "    # print(\"Point Cloud Shape:\", np.shape(point_cloud_global))\n",
    "\n",
    "    # Flip the point cloud\n",
    "    # pcd.transform([[1, 0, 0, 0],[0, -1, 0, 0],[0, 0, -1, 0],[0, 0, 0, 1]])\n",
    "\n",
    "    # Add color to the point cloud\n",
    "    pcd.colors = o3d.utility.Vector3dVector(rgb_values)\n",
    "\n",
    "    # if i == 0:\n",
    "    #     Rpcd = pcd\n",
    "            \n",
    "    # Save the point cloud to a file\n",
    "    # ply_file_path = os.path.join(ply_path, frame_id + \".ply\")    \n",
    "    # o3d.io.write_point_cloud(ply_file_path, pcd)\n",
    "\n",
    "    # Merge current point cloud with the overall point cloud\n",
    "    point_cloud += pcd\n",
    "\n",
    "    # Add camera Position\n",
    "    arrow = getArrowMesh()\n",
    "    arrow.transform(n_pose)\n",
    "    arrows += arrow\n",
    "\n",
    "    # Clear memory\n",
    "    depth_image = None\n",
    "    color_image = None\n",
    "    depth_array = None\n",
    "    color_array = None\n",
    "    pcd = None\n",
    "\n",
    "\n",
    "# Create an Open3D mesh representing coordinate axes\n",
    "axes = o3d.geometry.TriangleMesh.create_coordinate_frame(size=1, origin=[0, 0, 0])\n",
    "\n",
    "geometry_list = [point_cloud, arrows, axes] # axes, arrow\n",
    "o3d.visualization.draw_geometries(geometry_list)\n",
    "\n",
    "pcd = None\n",
    "axes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProjectionMatrixTorch(znear, zfar, fovX, fovY):\n",
    "    tanHalfFovY = math.tan((np.radians(fovY) / 2))\n",
    "    tanHalfFovX = math.tan((np.radians(fovX) / 2))\n",
    "\n",
    "    top = tanHalfFovY * znear\n",
    "    bottom = -top\n",
    "    right = tanHalfFovX * znear\n",
    "    left = -right\n",
    "\n",
    "    P = torch.zeros(4, 4)\n",
    "\n",
    "    z_sign = 1.0\n",
    "\n",
    "    P[0, 0] = 2.0 * znear / (right - left)\n",
    "    P[1, 1] = 2.0 * znear / (top - bottom)\n",
    "    P[0, 2] = (right + left) / (right - left)\n",
    "    P[1, 2] = (top + bottom) / (top - bottom)\n",
    "    P[3, 2] = z_sign\n",
    "    P[2, 2] = z_sign * zfar / (zfar - znear)\n",
    "    P[2, 3] = -(zfar * znear) / (zfar - znear)\n",
    "    return P\n",
    "\n",
    "def showRasterizedImageTorch(u,v, colors):\n",
    "    image_width, image_height = width, height\n",
    "    raster = torch.zeros((image_height, image_width, 3), dtype=torch.uint8 , device=colors.device)\n",
    "\n",
    "    # Create Indices\n",
    "    u_long = u.to(torch.long)\n",
    "    v_long = v.to(torch.long)\n",
    "\n",
    "    # print(u_long.min(), u_long.max())\n",
    "\n",
    "    # Store Points and Colors\n",
    "    raster[v_long, u_long] = (colors * 255).to(torch.uint8)\n",
    "    raster = raster.cpu().numpy()\n",
    "    plt.imshow(raster.astype(int))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_id = '0000'\n",
    "ply_file_path = os.path.join(ply_path, frame_id + \".ply\") \n",
    "Rpcd = o3d.io.read_point_cloud(ply_file_path)\n",
    "points = torch.tensor(Rpcd.points, dtype=torch.float32, device='cuda')\n",
    "colors = torch.tensor(Rpcd.colors, dtype=torch.float32, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterize Point Clouds\n",
    "C2W = extrinsics[frame_id]\n",
    "\n",
    "print(frame_id, C2W)\n",
    "print(width, height)\n",
    "\n",
    "# Calculate projection matrix from camera to world\n",
    "R = C2W[:3, :3]\n",
    "t = C2W[:3, 3]\n",
    "W2C = getWorld2View3(R, t)\n",
    "\n",
    "# Flip the Z Axis\n",
    "W2C = np.dot(np.array([[1, 0, 0, 0],[0, -1, 0, 0],[0, 0, -1, 0],[0, 0, 0, 1]]), W2C)\n",
    "\n",
    "image_width, image_height = width, height\n",
    "\n",
    "# cx = rgb_camera_params['cx,cy,fx,fy'][0]\n",
    "# cy = rgb_camera_params['cx,cy,fx,fy'][1]\n",
    "# fx = rgb_camera_params['cx,cy,fx,fy'][2] \n",
    "# fy = rgb_camera_params['cx,cy,fx,fy'][3]\n",
    "\n",
    "# # Normalized intrinsic parameters\n",
    "# fx_norm = 2 * fx / image_width\n",
    "# fy_norm = 2 * fy / image_height\n",
    "# cx_norm = (2 * cx - image_width) / image_width\n",
    "# cy_norm = (2 * cy - image_height) / image_height\n",
    "\n",
    "zNear, zFar = 0.01, 100\n",
    "\n",
    "# # Projection matrix\n",
    "# P1 = np.array([\n",
    "#     [fx_norm, 0, cx_norm, 0],\n",
    "#     [0, fy_norm, cy_norm, 0],\n",
    "#     [0, 0, -(zFar + zNear) / (zFar - zNear), -2 * zFar * zNear / (zFar - zNear)],\n",
    "#     [0, 0, -1, 0]\n",
    "# ])\n",
    "# FullProjection = np.dot(P1, W2C)\n",
    "\n",
    "fovX = rgb_camera_params['hFov']\n",
    "fovY = rgb_camera_params['vFov']\n",
    "\n",
    "print(\"fovX:\", fovX)\n",
    "print(\"fovY:\", fovY)\n",
    "\n",
    "P2 = getProjectionMatrixTorch(zNear, zFar, fovX, fovY)\n",
    "FullProjection = np.dot(P2, W2C)\n",
    "# print(np.dot(P1, W2C))\n",
    "# print(np.dot(FullProjection, W2C))\n",
    "projection_matrix = torch.tensor(FullProjection, dtype=torch.float32, device='cuda')\n",
    "\n",
    "print('W2C:',W2C)\n",
    "print('Projection:', P2)\n",
    "print('FP:',FullProjection)\n",
    "\n",
    "points_homogeneous = torch.cat((points, torch.ones(points.shape[0], 1, device=points.device)), dim=1)\n",
    "\n",
    "view_matrix = torch.tensor(W2C, dtype=torch.float32, device='cuda')\n",
    "view_points = torch.matmul(points_homogeneous, view_matrix.t())\n",
    "print('View Space Extent:', view_points[:,2].min(), view_points[:,2].max())\n",
    "\n",
    "projected_points_homogeneous = torch.matmul(points_homogeneous, projection_matrix.t()) # Clip space coordinates\n",
    "\n",
    "print('Clip Space:', projected_points_homogeneous.shape, projected_points_homogeneous.min(), projected_points_homogeneous.max())\n",
    "\n",
    "# Filter points outside clip space\n",
    "# mask = (projected_points_homogeneous[:, 0] >= -1) & (projected_points_homogeneous[:, 0] < 1) & \\\n",
    "#        (projected_points_homogeneous[:, 1] >= -1) & (projected_points_homogeneous[:, 1] < 1 ) & \\\n",
    "#        (projected_points_homogeneous[:, 2] >= -1) & (projected_points_homogeneous[:, 2] < 1) \n",
    "# projected_points_homogeneous = projected_points_homogeneous[mask]\n",
    "\n",
    "# Clip Space / Homogenous to NDC\n",
    "assert projected_points_homogeneous.shape[1] == 4\n",
    "# Extract x, y, z, w from the tensor\n",
    "x, y, z, w = projected_points_homogeneous[:, 0], projected_points_homogeneous[:, 1], projected_points_homogeneous[:, 2], projected_points_homogeneous[:, 3]\n",
    "x_ndc = x / w\n",
    "y_ndc = y / w\n",
    "z_ndc = z / w\n",
    "projected_points_NDC = torch.stack((x_ndc, y_ndc, z_ndc), dim=1)\n",
    "\n",
    "print('NDC:', projected_points_NDC.shape, projected_points_NDC.min(), projected_points_NDC.max())\n",
    "\n",
    "# NDC to Image Space [-1,1] to [0,1]\n",
    "projected_points_IS = (projected_points_NDC + 1) / 2\n",
    "print('IS:', projected_points_IS.shape, projected_points_IS.min(), projected_points_IS.max())\n",
    "\n",
    "\n",
    "# Filter points outside image space\n",
    "mask = (projected_points_IS[:, 0] >= 0) & (projected_points_IS[:, 0] <= 1) & \\\n",
    "       (projected_points_IS[:, 1] >= 0) & (projected_points_IS[:, 1] <= 1 )  & \\\n",
    "       (projected_points_IS[:, 2] >= 0) & (projected_points_IS[:, 2] <= 1)\n",
    "projected_points_IS = projected_points_IS[mask]\n",
    "\n",
    "points_filtered = points[mask]\n",
    "colors_filtered = colors[mask]\n",
    "\n",
    "print('Values: ', points_filtered.shape, colors_filtered.shape)\n",
    "\n",
    "u = projected_points_IS[:,0] * image_width\n",
    "v = projected_points_IS[:,1] * image_height\n",
    "showRasterizedImageTorch( u, v , colors_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frame_id)\n",
    "print(points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
