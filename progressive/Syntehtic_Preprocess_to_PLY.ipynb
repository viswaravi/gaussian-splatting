{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import os\n",
    "from os import makedirs\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from random import randint\n",
    "from scipy.spatial.transform import Rotation\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "\n",
    "np.set_printoptions(suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWorld2View2(R, t, translate=np.array([.0, .0, .0]), scale=1.0):\n",
    "    Rt = np.zeros((4, 4))\n",
    "    Rt[:3, :3] = R.transpose()\n",
    "    Rt[:3, 3] = t\n",
    "    Rt[3, 3] = 1.0\n",
    "\n",
    "    C2W = np.linalg.inv(Rt)\n",
    "    cam_center = C2W[:3, 3]\n",
    "    cam_center = (cam_center + translate) * scale\n",
    "    C2W[:3, 3] = cam_center\n",
    "    Rt = np.linalg.inv(C2W)\n",
    "    return np.float32(Rt)\n",
    "\n",
    "def getWorld2View4(R, t, translate=np.array([0.0, 0.0, 0.0]), scale=1.0):\n",
    "    # Scannet R,t = C2W\n",
    "    # Add Homogeneous Coordinate\n",
    "    Rt = np.eye(4)\n",
    "    Rt[:3, :3] = R\n",
    "    Rt[:3, 3] = t\n",
    "\n",
    "    # Add Translation and Scale\n",
    "    C2W = Rt\n",
    "    cam_center = C2W[:3, 3]\n",
    "    cam_center = (cam_center + translate) * scale\n",
    "    C2W[:3, 3] = cam_center\n",
    "\n",
    "    # Invert\n",
    "    R = C2W[:3, :3]\n",
    "    t = C2W[:3, 3]\n",
    "    R_inv = R.T\n",
    "    T_inv = -R_inv @ t\n",
    "    world_to_camera = np.eye(4)\n",
    "    world_to_camera[:3, :3] = R_inv\n",
    "    world_to_camera[:3, 3] = T_inv\n",
    "\n",
    "    return np.float32(world_to_camera)\n",
    "\n",
    "\n",
    "def getWorld2View(R, t):\n",
    "    # Scannet R,t = C2W\n",
    "    R_inv = R.T\n",
    "    T_inv = -R_inv @ t\n",
    "\n",
    "    world_to_camera = np.eye(4)\n",
    "    world_to_camera[:3, :3] = R_inv\n",
    "    world_to_camera[:3, 3] = T_inv\n",
    "\n",
    "    return world_to_camera\n",
    "\n",
    "\n",
    "def qvec2rotmat(qvec):\n",
    "    return np.array([\n",
    "        [1 - 2 * qvec[2]**2 - 2 * qvec[3]**2,\n",
    "         2 * qvec[1] * qvec[2] - 2 * qvec[0] * qvec[3],\n",
    "         2 * qvec[3] * qvec[1] + 2 * qvec[0] * qvec[2]],\n",
    "        [2 * qvec[1] * qvec[2] + 2 * qvec[0] * qvec[3],\n",
    "         1 - 2 * qvec[1]**2 - 2 * qvec[3]**2,\n",
    "         2 * qvec[2] * qvec[3] - 2 * qvec[0] * qvec[1]],\n",
    "        [2 * qvec[3] * qvec[1] - 2 * qvec[0] * qvec[2],\n",
    "         2 * qvec[2] * qvec[3] + 2 * qvec[0] * qvec[1],\n",
    "         1 - 2 * qvec[1]**2 - 2 * qvec[2]**2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'resolution': (1920, 1080),\n",
       "  'cx,cy,fx,fy': (959.5, 539.5, 960.0, 960.0),\n",
       "  'k1,k2,k3,p1,p2': (0.0, 0.0, 0.0, 0.0, 0.0),\n",
       "  'vFov': 58.7155,\n",
       "  'hFov': 90.0},\n",
       " {'resolution': (512, 424),\n",
       "  'cx,cy,fx,fy': (255.5, 211.5, 256.0, 256.0),\n",
       "  'k1,k2,k3,p1,p2': (0.0, 0.0, 0.0, 0.0, 0.0),\n",
       "  'vFov': 79.2579,\n",
       "  'hFov': 89.9999},\n",
       " {'RGB sensor': (0.0, 0.0, 0.0),\n",
       "  'RGB light source': (0.0, 0.3, 0.0),\n",
       "  'RGB light source size': (0.4, 0.4),\n",
       "  'Depth sensor': (0.03, 0.0, 0.0),\n",
       "  'Depth light source': (0.06, 0.0, 0.0),\n",
       "  'Depth light source size': (0.02, 0.02)})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Camera Intrinsics\n",
    "rgb_camera_params = {}\n",
    "depth_camera_params = {}\n",
    "relative_positions = {}\n",
    "\n",
    "def readRGBDConfig(config_file):\n",
    "    # Define dictionaries to hold camera parameters\n",
    "    # rgb_camera_params = {}\n",
    "    # depth_camera_params = {}\n",
    "    # relative_positions = {}\n",
    "    \n",
    "    with open(config_file, 'r') as file:\n",
    "        data = file.read().split('\\n\\n')\n",
    "    \n",
    "        # Read RGB camera parameters\n",
    "        rgb_data = data[0].split('\\n')\n",
    "        for line in rgb_data[1:4]:\n",
    "            key, value = line.split('=')\n",
    "            if ',' in value:\n",
    "                value = tuple(map(float, value.split(',')))\n",
    "            else:\n",
    "                value = tuple(map(int, value.split('x')))\n",
    "            rgb_camera_params[key] = value\n",
    "\n",
    "        vFOV, hFOV = rgb_data[4].split(',')\n",
    "        key, value = vFOV.split('=')\n",
    "        rgb_camera_params[key] = float(value.strip('°'))\n",
    "        key, value = hFOV.split('=')\n",
    "        rgb_camera_params[key.strip(' ')] = float(value.strip('°'))\n",
    "\n",
    "        # Read Depth camera parameters\n",
    "        depth_data = data[1].split('\\n')\n",
    "        for line in depth_data[1:4]:\n",
    "            key, value = line.split('=')\n",
    "            if ',' in value:\n",
    "                value = tuple(map(float, value.split(',')))\n",
    "            else:\n",
    "                value = tuple(map(int, value.split('x')))\n",
    "            depth_camera_params[key] = value\n",
    "\n",
    "        vFOV, hFOV = depth_data[4].split(',')\n",
    "        key, value = vFOV.split('=')\n",
    "        depth_camera_params[key] = float(value.strip('°'))\n",
    "        key, value = hFOV.split('=')\n",
    "        depth_camera_params[key.strip(' ')] = float(value.strip('°'))\n",
    "\n",
    "    \n",
    "        # Read relative positions of camera components\n",
    "        rel_pos_data = data[2].split('\\n')\n",
    "        for line in rel_pos_data[1:]:\n",
    "            key, value = line.split(': ')\n",
    "            value = tuple(map(float, value.strip('(').strip(')').split(',')))\n",
    "            relative_positions[key] = value\n",
    "\n",
    "        return rgb_camera_params, depth_camera_params, relative_positions\n",
    "    \n",
    "    # Access the loaded camera parameters\n",
    "    print(\"RGB Camera Parameters:\")\n",
    "    print(rgb_camera_params)\n",
    "    \n",
    "    print(\"\\nDepth Camera Parameters:\")\n",
    "    print(depth_camera_params)\n",
    "    \n",
    "    # print(\"\\nRelative Positions of Camera Components:\")\n",
    "    # print(relative_positions)0\n",
    "\n",
    "config_file = \"G:\\\\Universitat Siegen\\\\SA\\\\P-GPU\\\\Code\\\\gaussian-splatting\\\\data\\\\RGBD_Data\\\\SyntheticV1\\\\config\\\\configuration.txt\"\n",
    "readRGBDConfig(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB Camera Intrinsic Matrix:\n",
      "[[960.    0.  959.5]\n",
      " [  0.  960.  539.5]\n",
      " [  0.    0.    1. ]]\n",
      "[0. 0. 0. 0. 0.]\n",
      "TOF Camera Intrinsic Matrix:\n",
      "[[256.    0.  255.5]\n",
      " [  0.  256.  211.5]\n",
      " [  0.    0.    1. ]]\n",
      "[0. 0. 0. 0. 0.]\n",
      "Extrinsic Matrix: [[1.   0.   0.   0.03]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   0.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "cx = rgb_camera_params['cx,cy,fx,fy'][0]\n",
    "cy = rgb_camera_params['cx,cy,fx,fy'][1]\n",
    "fx = rgb_camera_params['cx,cy,fx,fy'][2] \n",
    "fy = rgb_camera_params['cx,cy,fx,fy'][3]\n",
    "\n",
    "k1 = rgb_camera_params['k1,k2,k3,p1,p2'][0]\n",
    "k2 = rgb_camera_params['k1,k2,k3,p1,p2'][1]\n",
    "k3 = rgb_camera_params['k1,k2,k3,p1,p2'][2]\n",
    "p1 = rgb_camera_params['k1,k2,k3,p1,p2'][3] \n",
    "p2 = rgb_camera_params['k1,k2,k3,p1,p2'][4]\n",
    "\n",
    "\n",
    "# RGB Camera Intrinsic Matrix\n",
    "K_rgb = np.array([[fx, 0, cx],\n",
    "                  [0, fy, cy],\n",
    "                  [0, 0, 1]])\n",
    "\n",
    "D_rgb = np.array([k1, k2, p1, p2, k3])\n",
    "\n",
    "cx_tof = depth_camera_params['cx,cy,fx,fy'][0]\n",
    "cy_tof = depth_camera_params['cx,cy,fx,fy'][1]\n",
    "fx_tof = depth_camera_params['cx,cy,fx,fy'][2] \n",
    "fy_tof = depth_camera_params['cx,cy,fx,fy'][3]\n",
    "\n",
    "k1_tof = rgb_camera_params['k1,k2,k3,p1,p2'][0]\n",
    "k2_tof = rgb_camera_params['k1,k2,k3,p1,p2'][1]\n",
    "k3_tof = rgb_camera_params['k1,k2,k3,p1,p2'][2]\n",
    "p1_tof = rgb_camera_params['k1,k2,k3,p1,p2'][3] \n",
    "p2_tof = rgb_camera_params['k1,k2,k3,p1,p2'][4]\n",
    "\n",
    "\n",
    "# TOF Camera Intrinsic Matrix\n",
    "K_tof = np.array([[fx_tof, 0, cx_tof],\n",
    "                  [0, fy_tof, cy_tof],\n",
    "                  [0, 0, 1]])\n",
    "\n",
    "D_tof = np.array([k1_tof, k2_tof, p1_tof, p2_tof, k3_tof])\n",
    "\n",
    "print(\"RGB Camera Intrinsic Matrix:\")\n",
    "print(K_rgb)\n",
    "print(D_rgb)\n",
    "print(\"TOF Camera Intrinsic Matrix:\")\n",
    "print(K_tof)\n",
    "print(D_tof)\n",
    "\n",
    "# Relative translation\n",
    "translation = np.array([0.03,0,0])  # Replace with your actual translation values\n",
    "\n",
    "# Relative rotation\n",
    "rotation = np.eye(3)  # Replace with your actual rotation matrix\n",
    "\n",
    "# Compose transformation matrix\n",
    "extrinsic_matrix = np.column_stack((rotation, translation))\n",
    "tof_extrinsic_matrix = np.row_stack((extrinsic_matrix, [0, 0, 0, 1]))\n",
    "\n",
    "print(\"Extrinsic Matrix:\", tof_extrinsic_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:00<00:00, 14697.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Camera Extrinsics\n",
    "pose_directory = \"G:\\\\Universitat Siegen\\\\SA\\\\P-GPU\\\\Code\\\\gaussian-splatting\\\\data\\\\RGBD_Data\\\\SyntheticV1\\\\config\"\n",
    "\n",
    "config_files = os.listdir(pose_directory)\n",
    "config_files.sort()\n",
    "\n",
    "extrinsics = {}\n",
    "\n",
    "# config_files[:5]\n",
    "# for config_index in tqdm(range(5)):\n",
    "for config_index in tqdm(range(len(config_files))):\n",
    "    file = config_files[config_index]\n",
    "    if file.startswith(\"campose-rgb-\"):\n",
    "        frame_id = file.split('-')[2].split('.')[0]\n",
    "        config_file = os.path.join(pose_directory, file)\n",
    "        # print(config_file)\n",
    "        \n",
    "        with open(config_file, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "            # Extracting position\n",
    "            position_str = lines[0].replace('position=', '').split('\\n')[0]\n",
    "            position = np.array([float(i) for i in position_str.strip('()').split(',')])\n",
    "\n",
    "            # Extracting rotation as a quaternion\n",
    "            rotation_str = lines[1].replace('rotation_as_quaternion=', '').split('\\n')[0]\n",
    "            rotation = np.array([float(i) for i in rotation_str.strip('()').split(',')])\n",
    "\n",
    "            # Extracting the 4x4 pose matrix\n",
    "            pose_str = lines[3:]\n",
    "            pose = np.array([[float(i) for i in row.strip('(').split(')')[0].split(',')] for row in pose_str if row != ''])    \n",
    "\n",
    "            # print('Position:', position)\n",
    "            # print('Rotation:',rotation)\n",
    "            # print('Pose:',pose)\n",
    "            extrinsics[frame_id] = pose\n",
    "\n",
    "print(\"Number of frames:\", len(extrinsics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 300\n",
      "Start index: 0\n"
     ]
    }
   ],
   "source": [
    "# Directory where your images are stored\n",
    "rgb_directory = \"G:\\\\Universitat Siegen\\\\SA\\\\P-GPU\\\\Code\\\\gaussian-splatting\\\\data\\\\RGBD_Data\\\\SyntheticV1\\\\rgb\"\n",
    "depth_directory = \"G:\\\\Universitat Siegen\\\\SA\\\\P-GPU\\\\Code\\\\gaussian-splatting\\\\data\\\\RGBD_Data\\\\SyntheticV1\\\\rgb\"\n",
    "ply_path = \"G:\\\\Universitat Siegen\\\\SA\\\\P-GPU\\\\Code\\\\gaussian-splatting\\\\data\\\\RGBD_Data\\\\SyntheticV1\\\\ply\"\n",
    "\n",
    "# Get the list of files in the directory\n",
    "files = os.listdir(depth_directory)\n",
    "\n",
    "# Sort the files to process depth and color images together\n",
    "files.sort()\n",
    "\n",
    "start_index = 0\n",
    "ply_files = os.listdir(ply_path)\n",
    "if len(ply_files) > 0:\n",
    "    start_index = int(ply_files[-1].split('.')[0])\n",
    "\n",
    "frame_step = 5\n",
    "# Loop through each pair of depth and color images\n",
    "# for i in tqdm(range(1)):\n",
    "\n",
    "depth_files = [file for file in files if file.startswith(\"gt-rgb-depth-\") ]\n",
    "# depth_files = depth_files[:100]\n",
    "\n",
    "print(\"Number of files:\", len(depth_files))\n",
    "print(\"Start index:\", start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPCD(points, colors):\n",
    "    # Extract the transformed X, Y, Z coordinates\n",
    "    X = points[0, :]\n",
    "    Y = points[1, :]\n",
    "    Z = points[2, :]\n",
    "    # Stack the global coordinates\n",
    "    point_cloud_points = np.vstack((X, Y, Z)).T\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(point_cloud_points)\n",
    "    # Add color to the point cloud\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    return pcd\n",
    "\n",
    "def getArrowMesh():\n",
    "    arrow = o3d.geometry.TriangleMesh.create_arrow(cylinder_radius=0.1,\n",
    "                                               cone_radius=0.20,\n",
    "                                               cylinder_height=0.25,\n",
    "                                               cone_height=0.1,\n",
    "                                               resolution=20,\n",
    "                                               cylinder_split=4,\n",
    "                                               cone_split=1)\n",
    "    arrow.compute_vertex_normals()\n",
    "    arrow.paint_uniform_color([1, 0, 0])\n",
    "    return arrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 300 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [04:02<00:00,  4.04s/it]\n"
     ]
    }
   ],
   "source": [
    "# Convert Depth Images to Point Clouds\n",
    "\n",
    "# Initialize an empty point cloud\n",
    "point_cloud = o3d.geometry.PointCloud()\n",
    "Rpcd = None\n",
    "\n",
    "print(start_index, len(depth_files), frame_step)\n",
    "# Using Numpy to create point clouds\n",
    "for i in tqdm(range(start_index, len(depth_files),frame_step)):\n",
    "# for i in range(start_index, len(files),frame_step):\n",
    "# for i in tqdm(range(1)):\n",
    "    depth_file_name = depth_files[i]\n",
    "    depth_file = os.path.join(depth_directory, depth_file_name)\n",
    "    \n",
    "    # Get the corresponding color image\n",
    "    color_file = os.path.join(rgb_directory, \"rgb-\" + depth_file_name[-8:])  # Assuming both files have corresponding indices\n",
    "    \n",
    "    frame_id = depth_file_name.split('-')[3].split('.')[0]\n",
    "\n",
    "    # Read the depth and color images\n",
    "    depth_image = cv2.imread(depth_file, cv2.IMREAD_ANYDEPTH)\n",
    "    # depth_image = cv2.resize(depth_image, (512, 424), interpolation=cv2.INTER_NEAREST)\n",
    "    color_image = cv2.imread(color_file)\n",
    "    # color_image = cv2.resize(color_image, (512, 424), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Convert images to numpy arrays\n",
    "    depth_array = np.asarray(depth_image)\n",
    "\n",
    "    color_array = np.asarray(color_image)\n",
    "    color_array = color_array[:, :, :3]\n",
    "\n",
    "    # print(np.shape(color_array), np.shape(depth_array))\n",
    "\n",
    "    # print(np.min(depth_array), np.max(depth_array))\n",
    "    # print(np.min(color_array), np.max(color_array))\n",
    "\n",
    "    width = color_array.shape[1]\n",
    "    height = color_array.shape[0]\n",
    "\n",
    "    # Intrinsic parameters of the camera (you may need to adjust these values)\n",
    "    cx = rgb_camera_params['cx,cy,fx,fy'][0]\n",
    "    cy = rgb_camera_params['cx,cy,fx,fy'][1]\n",
    "    fx = rgb_camera_params['cx,cy,fx,fy'][2] \n",
    "    fy = rgb_camera_params['cx,cy,fx,fy'][3]\n",
    "\n",
    "    # vFov = rgb_camera_params['vFov']\n",
    "    # hFov = rgb_camera_params['hFov']\n",
    "    # print(\"vFov:\", vFov)\n",
    "    # print(\"hFov:\", hFov)\n",
    "    # x_corr = math.tan(math.radians(hFov/2)) / (1920/2)\n",
    "    # y_corr = math.tan(math.radians(vFov/2)) / (1080/2)\n",
    "\n",
    "    # print(\"Frame:\", frame_id)\n",
    "    assert frame_id in extrinsics, \"No pose found for frame \" + frame_id\n",
    "\n",
    "    pose = extrinsics[frame_id]\n",
    "    # pose = np.linalg.inv(pose)\n",
    "\n",
    "    # print(depth_file)\n",
    "    # print(\"Pose:\", pose)\n",
    "\n",
    "    # Convert depth to 3D coordinates in camera coordinates\n",
    "    u, v = np.meshgrid(np.arange(width), np.arange(height-1, -1, -1))\n",
    "    u = u.flatten()\n",
    "    v = v.flatten()\n",
    "    depth_image = depth_image.astype(np.float32)\n",
    "    depth_scale = 1.0 / 10000.0\n",
    "    depth = depth_image.flatten() * depth_scale\n",
    "    X = ((u - cx) * depth / fx)\n",
    "    Y = ((v - cy) * depth / fy)\n",
    "    Z = -depth\n",
    "\n",
    "    # print(np.min(depth), np.max(depth))\n",
    "\n",
    "    rgb_values = color_array.reshape((-1, 3)) / 255.0\n",
    "    \n",
    "    # Stack the 3D coordinates\n",
    "    points_camera = np.vstack((X, Y, Z, np.ones_like(X)))\n",
    "\n",
    "    # Transform to global coordinates\n",
    "    points_global = np.dot(pose, points_camera)\n",
    "\n",
    "    # Transform points again to camera coordinates and check\n",
    "    if False:\n",
    "        # Extract the transformed X, Y, Z coordinates\n",
    "        R = pose[:3, :3]\n",
    "        t = pose[:3, 3]\n",
    "        W2C = getWorld2View(R, t)\n",
    "        points_camera_again = np.dot(W2C,points_global)\n",
    "        pcd = getPCD(points_camera_again, rgb_values)\n",
    "        # Flip the Z Axis\n",
    "        pcd.transform([[1, 0, 0, 0],[0, -1, 0, 0],[0, 0, -1, 0],[0, 0, 0, 1]])\n",
    "        point_cloud += pcd\n",
    "\n",
    "    if False:\n",
    "        pcd = getPCD(points_camera, rgb_values)\n",
    "        point_cloud += pcd\n",
    "\n",
    "    # Extract the transformed X, Y, Z coordinates\n",
    "    X_global = points_global[0, :] / points_global[3 , :]\n",
    "    Y_global = points_global[1, :] / points_global[3 , :]\n",
    "    Z_global = points_global[2, :] / points_global[3 , :]\n",
    "    # Stack the global coordinates\n",
    "    point_cloud_global = np.vstack((X_global, Y_global, Z_global)).T\n",
    "\n",
    "    # Create an Open3D PointCloud\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    # Set the points in the PointCloud\n",
    "    pcd.points = o3d.utility.Vector3dVector(point_cloud_global)\n",
    "\n",
    "    # print(\"Point Cloud Shape:\", np.shape(point_cloud_global))\n",
    "\n",
    "    # Flip the point cloud\n",
    "    # pcd.transform([[1, 0, 0, 0],[0, -1, 0, 0],[0, 0, -1, 0],[0, 0, 0, 1]])\n",
    "\n",
    "    # Add color to the point cloud\n",
    "    pcd.colors = o3d.utility.Vector3dVector(rgb_values)\n",
    "\n",
    "    if i == 0:\n",
    "        Rpcd = pcd\n",
    "            \n",
    "    # Save the point cloud to a file\n",
    "    ply_file_path = os.path.join(ply_path, frame_id + \".ply\")    \n",
    "    o3d.io.write_point_cloud(ply_file_path, pcd)\n",
    "\n",
    "    # Merge current point cloud with the overall point cloud\n",
    "    point_cloud += pcd\n",
    "\n",
    "    # Add camera Position\n",
    "    arrow = getArrowMesh()\n",
    "    arrow.transform(pose)\n",
    "\n",
    "\n",
    "    # Clear memory\n",
    "    depth_image = None\n",
    "    color_image = None\n",
    "    depth_array = None\n",
    "    color_array = None\n",
    "    pcd = None\n",
    "\n",
    "# Create an Open3D mesh representing coordinate axes\n",
    "axes = o3d.geometry.TriangleMesh.create_coordinate_frame(size=1, origin=[0, 0, 0])\n",
    "\n",
    "geometry_list = [point_cloud, arrow, axes] # axes, arrow\n",
    "# o3d.visualization.draw_geometries(geometry_list)\n",
    "\n",
    "pcd = None\n",
    "axes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProjectionMatrixTorch(znear, zfar, fovX, fovY):\n",
    "    tanHalfFovY = math.tan((np.radians(fovY) / 2))\n",
    "    tanHalfFovX = math.tan((np.radians(fovX) / 2))\n",
    "\n",
    "    top = tanHalfFovY * znear\n",
    "    bottom = -top\n",
    "    right = tanHalfFovX * znear\n",
    "    left = -right\n",
    "\n",
    "    P = torch.zeros(4, 4)\n",
    "\n",
    "    z_sign = 1.0\n",
    "\n",
    "    P[0, 0] = 2.0 * znear / (right - left)\n",
    "    P[1, 1] = 2.0 * znear / (top - bottom)\n",
    "    P[0, 2] = (right + left) / (right - left)\n",
    "    P[1, 2] = (top + bottom) / (top - bottom)\n",
    "    P[3, 2] = z_sign\n",
    "    P[2, 2] = z_sign * zfar / (zfar - znear)\n",
    "    P[2, 3] = -(zfar * znear) / (zfar - znear)\n",
    "    return P\n",
    "\n",
    "def showRasterizedImageTorch(u,v, colors):\n",
    "    image_width, image_height = width, height\n",
    "    raster = torch.zeros((image_height, image_width, 3), dtype=torch.uint8 , device=colors.device)\n",
    "\n",
    "    # Create Indices\n",
    "    u_long = u.to(torch.long)\n",
    "    v_long = v.to(torch.long)\n",
    "\n",
    "    # print(u_long.min(), u_long.max())\n",
    "\n",
    "    # Store Points and Colors\n",
    "    raster[v_long, u_long] = (colors * 255).to(torch.uint8)\n",
    "    raster = raster.cpu().numpy()\n",
    "    plt.imshow(raster.astype(int))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.tensor(Rpcd.points, dtype=torch.float32, device='cuda')\n",
    "colors = torch.tensor(Rpcd.colors, dtype=torch.float32, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0122,  0.9665,  5.1830],\n",
       "        [ 3.0122,  0.9665,  5.1860],\n",
       "        [ 3.0122,  0.9665,  5.1891],\n",
       "        ...,\n",
       "        [-0.4822,  0.0283,  2.8761],\n",
       "        [-0.4833,  0.0283,  2.8761],\n",
       "        [-0.4843,  0.0283,  2.8761]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000 [[-1.          0.00000004 -0.00000008  0.5       ]\n",
      " [-0.          0.866025    0.5         1.        ]\n",
      " [ 0.00000009  0.5        -0.866025    2.3       ]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "1920 1080\n",
      "fovX: 90.0\n",
      "fovY: 58.7155\n",
      "W2C: [[-1.         -0.          0.00000009  0.4999998 ]\n",
      " [ 0.00000004  0.866025    0.5        -2.016025  ]\n",
      " [-0.00000008  0.5        -0.866025    1.4918575 ]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Projection: tensor([[ 1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  1.7778,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.0001, -0.0100],\n",
      "        [ 0.0000,  0.0000,  1.0000,  0.0000]])\n",
      "FP: [[-1.         -0.          0.00000009  0.4999998 ]\n",
      " [ 0.00000008  1.5396001   0.888889   -3.5840452 ]\n",
      " [-0.00000008  0.50005    -0.8661116   1.4820058 ]\n",
      " [-0.00000008  0.5        -0.866025    1.4918575 ]]\n",
      "torch.Size([2073600, 4]) tensor(-8.0445, device='cuda:0') tensor(3.0147, device='cuda:0')\n",
      "NDC: torch.Size([2073600, 3]) tensor(-2.1058, device='cuda:0') tensor(0.9989, device='cuda:0')\n",
      "IS: torch.Size([2073600, 3]) tensor(-0.5529, device='cuda:0') tensor(0.9995, device='cuda:0')\n",
      "Values:  torch.Size([0, 3]) torch.Size([0, 3])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rasterize Point Clouds\n",
    "C2W = extrinsics[frame_id]\n",
    "\n",
    "print(frame_id, C2W)\n",
    "print(width, height)\n",
    "\n",
    "# Calculate projection matrix from camera to world\n",
    "R = C2W[:3, :3]\n",
    "t = C2W[:3, 3]\n",
    "W2C = getWorld2View4(R, t)\n",
    "\n",
    "# Flip the Z Axis\n",
    "# W2C = np.dot(np.array([[1, 0, 0, 0],[0, -1, 0, 0],[0, 0, -1, 0],[0, 0, 0, 1]]), W2C)\n",
    "\n",
    "image_width, image_height = width, height\n",
    "\n",
    "# cx = rgb_camera_params['cx,cy,fx,fy'][0]\n",
    "# cy = rgb_camera_params['cx,cy,fx,fy'][1]\n",
    "# fx = rgb_camera_params['cx,cy,fx,fy'][2] \n",
    "# fy = rgb_camera_params['cx,cy,fx,fy'][3]\n",
    "\n",
    "# # Normalized intrinsic parameters\n",
    "# fx_norm = 2 * fx / image_width\n",
    "# fy_norm = 2 * fy / image_height\n",
    "# cx_norm = (2 * cx - image_width) / image_width\n",
    "# cy_norm = (2 * cy - image_height) / image_height\n",
    "\n",
    "zNear, zFar = 0.01, 100\n",
    "\n",
    "# # Projection matrix\n",
    "# P1 = np.array([\n",
    "#     [fx_norm, 0, cx_norm, 0],\n",
    "#     [0, fy_norm, cy_norm, 0],\n",
    "#     [0, 0, -(zFar + zNear) / (zFar - zNear), -2 * zFar * zNear / (zFar - zNear)],\n",
    "#     [0, 0, -1, 0]\n",
    "# ])\n",
    "# FullProjection = np.dot(P1, W2C)\n",
    "\n",
    "fovX = rgb_camera_params['hFov']\n",
    "fovY = rgb_camera_params['vFov']\n",
    "\n",
    "print(\"fovX:\", fovX)\n",
    "print(\"fovY:\", fovY)\n",
    "\n",
    "P2 = getProjectionMatrixTorch(zNear, zFar, fovX, fovY)\n",
    "FullProjection = np.dot(P2, W2C)\n",
    "# print(np.dot(P1, W2C))\n",
    "# print(np.dot(FullProjection, W2C))\n",
    "\n",
    "print('W2C:',W2C)\n",
    "print('Projection:', P2)\n",
    "print('FP:',FullProjection)\n",
    "\n",
    "projection_matrix = torch.tensor(FullProjection, dtype=torch.float32, device='cuda')\n",
    "\n",
    "\n",
    "points_homogeneous = torch.cat((points, torch.ones(points.shape[0], 1, device=points.device)), dim=1)\n",
    "print(points_homogeneous.shape, points_homogeneous.min(), points_homogeneous.max())\n",
    "\n",
    "projected_points_homogeneous = torch.matmul(points_homogeneous, projection_matrix.t()) # Clip space coordinates\n",
    "\n",
    "# print('Clip Space:', projected_points_homogeneous.shape, projected_points_homogeneous.min(), projected_points_homogeneous.max())\n",
    "\n",
    "# # # Filter points outside clip space\n",
    "# # mask = (projected_points_homogeneous[:, 0] >= -1) & (projected_points_homogeneous[:, 0] < 1) & \\\n",
    "# #        (projected_points_homogeneous[:, 1] >= -1) & (projected_points_homogeneous[:, 1] < 1 ) # & \\\n",
    "#        # (projected_points_homogeneous[:, 2] >= -1) & (projected_points_homogeneous[:, 2] < 1) // Removes points closer to camera so not included\n",
    "# # projected_points_homogeneous = projected_points_homogeneous[mask]\n",
    "\n",
    "# Clip Space / Homogenous to NDC\n",
    "assert projected_points_homogeneous.shape[1] == 4\n",
    "# Extract x, y, z, w from the tensor\n",
    "x, y, z, w = projected_points_homogeneous[:, 0], projected_points_homogeneous[:, 1], projected_points_homogeneous[:, 2], projected_points_homogeneous[:, 3]\n",
    "x_ndc = x / w\n",
    "y_ndc = y / w\n",
    "z_ndc = z / w\n",
    "projected_points_NDC = torch.stack((x_ndc, y_ndc, z_ndc), dim=1)\n",
    "\n",
    "print('NDC:', projected_points_NDC.shape, projected_points_NDC.min(), projected_points_NDC.max())\n",
    "\n",
    "# NDC to Image Space [-1,1] to [0,1]\n",
    "projected_points_IS = (projected_points_NDC + 1) / 2\n",
    "print('IS:', projected_points_IS.shape, projected_points_IS.min(), projected_points_IS.max())\n",
    "\n",
    "\n",
    "# Filter points outside image space\n",
    "mask = (projected_points_IS[:, 0] >= 0) & (projected_points_IS[:, 0] <= 1) & \\\n",
    "       (projected_points_IS[:, 1] >= 0) & (projected_points_IS[:, 1] <= 1 ) & \\\n",
    "       (projected_points_IS[:, 2] >= 0) & (projected_points_IS[:, 2] <= 1)\n",
    "\n",
    "projected_points_IS = projected_points_IS[mask]\n",
    "\n",
    "\n",
    "points_filtered = points[mask]\n",
    "colors_filtered = colors[mask]\n",
    "\n",
    "print('Values: ', points_filtered.shape, colors_filtered.shape)\n",
    "\n",
    "u = projected_points_IS[:,0] * image_width\n",
    "v = projected_points_IS[:,1] * image_height\n",
    "showRasterizedImageTorch( u, v , colors_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000\n",
      "tensor([[ 3.0122,  0.9665,  5.1830],\n",
      "        [ 3.0122,  0.9665,  5.1860],\n",
      "        [ 3.0122,  0.9665,  5.1891],\n",
      "        ...,\n",
      "        [-0.4822,  0.0283,  2.8761],\n",
      "        [-0.4833,  0.0283,  2.8761],\n",
      "        [-0.4843,  0.0283,  2.8761]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(frame_id)\n",
    "print(points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
