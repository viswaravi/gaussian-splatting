{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import os\n",
    "from os import makedirs\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from random import randint\n",
    "from scipy.spatial.transform import Rotation\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "\n",
    "np.set_printoptions(suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWorld2View2(R, t, translate=np.array([.0, .0, .0]), scale=1.0):\n",
    "    Rt = np.zeros((4, 4))\n",
    "    Rt[:3, :3] = R.transpose()\n",
    "    Rt[:3, 3] = t\n",
    "    Rt[3, 3] = 1.0\n",
    "\n",
    "    C2W = np.linalg.inv(Rt)\n",
    "    cam_center = C2W[:3, 3]\n",
    "    cam_center = (cam_center + translate) * scale\n",
    "    C2W[:3, 3] = cam_center\n",
    "    Rt = np.linalg.inv(C2W)\n",
    "    return np.float32(Rt)\n",
    "\n",
    "def getWorld2View3(R, t, translate=np.array([0.0, 0.0, 0.0]), scale=1.0):\n",
    "    # Add Homogeneous Coordinate\n",
    "    Rt = np.eye(4)\n",
    "    Rt[:3, :3] = R\n",
    "    Rt[:3, 3] = t\n",
    "\n",
    "    # Add Translation and Scale\n",
    "    C2W = Rt\n",
    "    cam_center = C2W[:3, 3]\n",
    "    cam_center = (cam_center + translate) * scale\n",
    "    C2W[:3, 3] = cam_center\n",
    "\n",
    "    # Invert\n",
    "    R = C2W[:3, :3]\n",
    "    t = C2W[:3, 3]\n",
    "    R_inv = R.T\n",
    "    T_inv = -R_inv @ t\n",
    "    world_to_camera = np.eye(4)\n",
    "    world_to_camera[:3, :3] = R_inv\n",
    "    world_to_camera[:3, 3] = T_inv\n",
    "\n",
    "    return np.float32(world_to_camera)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'resolution': (1920, 1080),\n",
       "  'cx,cy,fx,fy': (959.5, 539.5, 960.0, 960.0),\n",
       "  'k1,k2,k3,p1,p2': (0.0, 0.0, 0.0, 0.0, 0.0),\n",
       "  'vFov': 58.7155,\n",
       "  'hFov': 90.0},\n",
       " {'resolution': (512, 424),\n",
       "  'cx,cy,fx,fy': (255.5, 211.5, 256.0, 256.0),\n",
       "  'k1,k2,k3,p1,p2': (0.0, 0.0, 0.0, 0.0, 0.0),\n",
       "  'vFov': 79.2579,\n",
       "  'hFov': 89.9999},\n",
       " {'RGB sensor': (0.0, 0.0, 0.0),\n",
       "  'RGB light source': (0.0, 0.3, 0.0),\n",
       "  'RGB light source size': (0.4, 0.4),\n",
       "  'Depth sensor': (0.03, 0.0, 0.0),\n",
       "  'Depth light source': (0.06, 0.0, 0.0),\n",
       "  'Depth light source size': (0.02, 0.02)})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Camera Intrinsics\n",
    "rgb_camera_params = {}\n",
    "depth_camera_params = {}\n",
    "relative_positions = {}\n",
    "\n",
    "def readRGBDConfig(config_file):\n",
    "    # Define dictionaries to hold camera parameters\n",
    "    # rgb_camera_params = {}\n",
    "    # depth_camera_params = {}\n",
    "    # relative_positions = {}\n",
    "    \n",
    "    with open(config_file, 'r') as file:\n",
    "        data = file.read().split('\\n\\n')\n",
    "    \n",
    "        # Read RGB camera parameters\n",
    "        rgb_data = data[0].split('\\n')\n",
    "        for line in rgb_data[1:4]:\n",
    "            key, value = line.split('=')\n",
    "            if ',' in value:\n",
    "                value = tuple(map(float, value.split(',')))\n",
    "            else:\n",
    "                value = tuple(map(int, value.split('x')))\n",
    "            rgb_camera_params[key] = value\n",
    "\n",
    "        vFOV, hFOV = rgb_data[4].split(',')\n",
    "        key, value = vFOV.split('=')\n",
    "        rgb_camera_params[key] = float(value.strip('°'))\n",
    "        key, value = hFOV.split('=')\n",
    "        rgb_camera_params[key.strip(' ')] = float(value.strip('°'))\n",
    "\n",
    "        # Read Depth camera parameters\n",
    "        depth_data = data[1].split('\\n')\n",
    "        for line in depth_data[1:4]:\n",
    "            key, value = line.split('=')\n",
    "            if ',' in value:\n",
    "                value = tuple(map(float, value.split(',')))\n",
    "            else:\n",
    "                value = tuple(map(int, value.split('x')))\n",
    "            depth_camera_params[key] = value\n",
    "\n",
    "        vFOV, hFOV = depth_data[4].split(',')\n",
    "        key, value = vFOV.split('=')\n",
    "        depth_camera_params[key] = float(value.strip('°'))\n",
    "        key, value = hFOV.split('=')\n",
    "        depth_camera_params[key.strip(' ')] = float(value.strip('°'))\n",
    "\n",
    "    \n",
    "        # Read relative positions of camera components\n",
    "        rel_pos_data = data[2].split('\\n')\n",
    "        for line in rel_pos_data[1:]:\n",
    "            key, value = line.split(': ')\n",
    "            value = tuple(map(float, value.strip('(').strip(')').split(',')))\n",
    "            relative_positions[key] = value\n",
    "\n",
    "        return rgb_camera_params, depth_camera_params, relative_positions\n",
    "    \n",
    "    # Access the loaded camera parameters\n",
    "    print(\"RGB Camera Parameters:\")\n",
    "    print(rgb_camera_params)\n",
    "    \n",
    "    print(\"\\nDepth Camera Parameters:\")\n",
    "    print(depth_camera_params)\n",
    "    \n",
    "    # print(\"\\nRelative Positions of Camera Components:\")\n",
    "    # print(relative_positions)0\n",
    "\n",
    "config_file = \"G:\\\\Universitat Siegen\\\\SA\\\\P-GPU\\\\Code\\\\gaussian-splatting\\\\data\\\\RGBD_Data\\\\SyntheticV1\\\\config\\\\configuration.txt\"\n",
    "readRGBDConfig(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB Camera Intrinsic Matrix:\n",
      "[[960.    0.  959.5]\n",
      " [  0.  960.  539.5]\n",
      " [  0.    0.    1. ]]\n",
      "[0. 0. 0. 0. 0.]\n",
      "TOF Camera Intrinsic Matrix:\n",
      "[[256.    0.  255.5]\n",
      " [  0.  256.  211.5]\n",
      " [  0.    0.    1. ]]\n",
      "[0. 0. 0. 0. 0.]\n",
      "Extrinsic Matrix: [[1.   0.   0.   0.03]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   0.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "cx = rgb_camera_params['cx,cy,fx,fy'][0]\n",
    "cy = rgb_camera_params['cx,cy,fx,fy'][1]\n",
    "fx = rgb_camera_params['cx,cy,fx,fy'][2] \n",
    "fy = rgb_camera_params['cx,cy,fx,fy'][3]\n",
    "\n",
    "k1 = rgb_camera_params['k1,k2,k3,p1,p2'][0]\n",
    "k2 = rgb_camera_params['k1,k2,k3,p1,p2'][1]\n",
    "k3 = rgb_camera_params['k1,k2,k3,p1,p2'][2]\n",
    "p1 = rgb_camera_params['k1,k2,k3,p1,p2'][3] \n",
    "p2 = rgb_camera_params['k1,k2,k3,p1,p2'][4]\n",
    "\n",
    "\n",
    "# RGB Camera Intrinsic Matrix\n",
    "K_rgb = np.array([[fx, 0, cx],\n",
    "                  [0, fy, cy],\n",
    "                  [0, 0, 1]])\n",
    "\n",
    "D_rgb = np.array([k1, k2, p1, p2, k3])\n",
    "\n",
    "cx_tof = depth_camera_params['cx,cy,fx,fy'][0]\n",
    "cy_tof = depth_camera_params['cx,cy,fx,fy'][1]\n",
    "fx_tof = depth_camera_params['cx,cy,fx,fy'][2] \n",
    "fy_tof = depth_camera_params['cx,cy,fx,fy'][3]\n",
    "\n",
    "k1_tof = rgb_camera_params['k1,k2,k3,p1,p2'][0]\n",
    "k2_tof = rgb_camera_params['k1,k2,k3,p1,p2'][1]\n",
    "k3_tof = rgb_camera_params['k1,k2,k3,p1,p2'][2]\n",
    "p1_tof = rgb_camera_params['k1,k2,k3,p1,p2'][3] \n",
    "p2_tof = rgb_camera_params['k1,k2,k3,p1,p2'][4]\n",
    "\n",
    "\n",
    "# TOF Camera Intrinsic Matrix\n",
    "K_tof = np.array([[fx_tof, 0, cx_tof],\n",
    "                  [0, fy_tof, cy_tof],\n",
    "                  [0, 0, 1]])\n",
    "\n",
    "D_tof = np.array([k1_tof, k2_tof, p1_tof, p2_tof, k3_tof])\n",
    "\n",
    "print(\"RGB Camera Intrinsic Matrix:\")\n",
    "print(K_rgb)\n",
    "print(D_rgb)\n",
    "print(\"TOF Camera Intrinsic Matrix:\")\n",
    "print(K_tof)\n",
    "print(D_tof)\n",
    "\n",
    "# Relative translation\n",
    "translation = np.array([0.03,0,0])  # Replace with your actual translation values\n",
    "\n",
    "# Relative rotation\n",
    "rotation = np.eye(3)  # Replace with your actual rotation matrix\n",
    "\n",
    "# Compose transformation matrix\n",
    "extrinsic_matrix = np.column_stack((rotation, translation))\n",
    "tof_extrinsic_matrix = np.row_stack((extrinsic_matrix, [0, 0, 0, 1]))\n",
    "\n",
    "print(\"Extrinsic Matrix:\", tof_extrinsic_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:03<00:00, 185.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Camera Extrinsics\n",
    "pose_directory = \"G:\\\\Universitat Siegen\\\\SA\\\\P-GPU\\\\Code\\\\gaussian-splatting\\\\data\\\\RGBD_Data\\\\SyntheticV1\\\\config\"\n",
    "\n",
    "config_files = os.listdir(pose_directory)\n",
    "config_files.sort()\n",
    "\n",
    "extrinsics = {}\n",
    "\n",
    "# config_files[:5]\n",
    "# for config_index in tqdm(range(5)):\n",
    "for config_index in tqdm(range(len(config_files))):\n",
    "    file = config_files[config_index]\n",
    "    if file.startswith(\"campose-rgb-\"):\n",
    "        frame_id = file.split('-')[2].split('.')[0]\n",
    "        config_file = os.path.join(pose_directory, file)\n",
    "        # print(config_file)\n",
    "        \n",
    "        with open(config_file, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "            # Extracting position\n",
    "            position_str = lines[0].replace('position=', '').split('\\n')[0]\n",
    "            position = np.array([float(i) for i in position_str.strip('()').split(',')])\n",
    "\n",
    "            # Extracting rotation as a quaternion\n",
    "            rotation_str = lines[1].replace('rotation_as_quaternion=', '').split('\\n')[0]\n",
    "            rotation = np.array([float(i) for i in rotation_str.strip('()').split(',')])\n",
    "\n",
    "            # Extracting the 4x4 pose matrix\n",
    "            pose_str = lines[3:]\n",
    "            pose = np.array([[float(i) for i in row.strip('(').split(')')[0].split(',')] for row in pose_str if row != ''])    \n",
    "\n",
    "            # print('Position:', position)\n",
    "            # print('Rotation:',rotation)\n",
    "            # print('Pose:',pose)\n",
    "            extrinsics[frame_id] = pose\n",
    "\n",
    "print(\"Number of frames:\", len(extrinsics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 300\n",
      "Start index: 295\n"
     ]
    }
   ],
   "source": [
    "# Directory where your images are stored\n",
    "rgb_directory = \"G:\\\\Universitat Siegen\\\\SA\\\\P-GPU\\\\Code\\\\gaussian-splatting\\\\data\\\\RGBD_Data\\\\SyntheticV1\\\\rgb\"\n",
    "depth_directory = \"G:\\\\Universitat Siegen\\\\SA\\\\P-GPU\\\\Code\\\\gaussian-splatting\\\\data\\\\RGBD_Data\\\\SyntheticV1\\\\rgb\"\n",
    "ply_path = \"G:\\\\Universitat Siegen\\\\SA\\\\P-GPU\\\\Code\\\\gaussian-splatting\\\\data\\\\RGBD_Data\\\\SyntheticV1\\\\ply\"\n",
    "\n",
    "# Get the list of files in the directory\n",
    "files = os.listdir(depth_directory)\n",
    "\n",
    "# Sort the files to process depth and color images together\n",
    "files.sort()\n",
    "\n",
    "start_index = 0\n",
    "ply_files = os.listdir(ply_path)\n",
    "if len(ply_files) > 0:\n",
    "    start_index = int(ply_files[-1].split('.')[0])\n",
    "\n",
    "frame_step = 5\n",
    "# Loop through each pair of depth and color images\n",
    "# for i in tqdm(range(1)):\n",
    "\n",
    "depth_files = [file for file in files if file.startswith(\"gt-rgb-depth-\") ]\n",
    "# depth_files = depth_files[:100]\n",
    "\n",
    "print(\"Number of files:\", len(depth_files))\n",
    "print(\"Start index:\", start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPCD(points, colors):\n",
    "    # Extract the transformed X, Y, Z coordinates\n",
    "    X = points[0, :]\n",
    "    Y = points[1, :]\n",
    "    Z = points[2, :]\n",
    "    # Stack the global coordinates\n",
    "    point_cloud_points = np.vstack((X, Y, Z)).T\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(point_cloud_points)\n",
    "    # Add color to the point cloud\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    return pcd\n",
    "\n",
    "def getArrowMesh():\n",
    "    arrow = o3d.geometry.TriangleMesh.create_arrow(cylinder_radius=0.1,\n",
    "                                               cone_radius=0.20,\n",
    "                                               cylinder_height=0.25,\n",
    "                                               cone_height=0.1,\n",
    "                                               resolution=20,\n",
    "                                               cylinder_split=4,\n",
    "                                               cone_split=1)\n",
    "    arrow.compute_vertex_normals()\n",
    "    arrow.paint_uniform_color([1, 0, 0])\n",
    "    return arrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 300 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "# Convert Depth Images to Point Clouds\n",
    "\n",
    "# Initialize an empty point cloud\n",
    "point_cloud = o3d.geometry.PointCloud()\n",
    "Rpcd = None\n",
    "resolution_scale = 2\n",
    "arrows = o3d.geometry.TriangleMesh()\n",
    "\n",
    "print(start_index, len(depth_files), frame_step)\n",
    "# Using Numpy to create point clouds\n",
    "# for i in tqdm(range(start_index, len(depth_files),frame_step)):\n",
    "# for i in range(start_index, len(files),frame_step):\n",
    "# for i in tqdm(range(0,6,2)):\n",
    "for i in tqdm(range(10)):\n",
    "    depth_file_name = depth_files[i]\n",
    "    depth_file = os.path.join(depth_directory, depth_file_name)\n",
    "    \n",
    "    # Get the corresponding color image\n",
    "    color_file = os.path.join(rgb_directory, \"rgb-\" + depth_file_name[-8:])  # Assuming both files have corresponding indices\n",
    "    color_image = cv2.imread(color_file)\n",
    "    depth_image = cv2.imread(depth_file, cv2.IMREAD_ANYDEPTH)\n",
    "    \n",
    "    frame_id = depth_file_name.split('-')[3].split('.')[0]\n",
    "\n",
    "    # Read the depth and color images\n",
    "    original_width, original_height = color_image.shape[1], color_image.shape[0]\n",
    "    width = int(original_width / resolution_scale)\n",
    "    height = int(original_height * (width / original_width))\n",
    "\n",
    "    # print(\"Width:\", width, \"Height:\", height)\n",
    "\n",
    "    # Resize the images\n",
    "    depth_image = cv2.resize(depth_image, (width, height), interpolation=cv2.INTER_NEAREST)\n",
    "    color_image = cv2.resize(color_image, (width, height), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Convert images to numpy arrays\n",
    "    depth_array = np.asarray(depth_image)\n",
    "    color_array = np.asarray(color_image)\n",
    "    color_array = color_array[:, :, :3]\n",
    "\n",
    "    # print(np.shape(color_array), np.shape(depth_array))\n",
    "    # print(np.min(depth_array), np.max(depth_array))\n",
    "\n",
    "    scale_factor_width = width / original_width\n",
    "    scale_factor_height = height / original_height\n",
    "\n",
    "    \n",
    "\n",
    "    # Intrinsic parameters of the camera (you may need to adjust these values)\n",
    "    cx = rgb_camera_params['cx,cy,fx,fy'][0] * scale_factor_width\n",
    "    cy = rgb_camera_params['cx,cy,fx,fy'][1] * scale_factor_height\n",
    "    fx = rgb_camera_params['cx,cy,fx,fy'][2] * scale_factor_width\n",
    "    fy = rgb_camera_params['cx,cy,fx,fy'][3] * scale_factor_height\n",
    "\n",
    "    # print(\"cx:\", cx, \"cy:\", cy, \"fx:\", fx, \"fy:\", fy)\n",
    "    # vFov = rgb_camera_params['vFov']\n",
    "    # hFov = rgb_camera_params['hFov']\n",
    "    # print(\"vFov:\", vFov)\n",
    "    # print(\"hFov:\", hFov)\n",
    "    # x_corr = math.tan(math.radians(hFov/2)) / (1920/2)\n",
    "    # y_corr = math.tan(math.radians(vFov/2)) / (1080/2)\n",
    "\n",
    "    # print(\"Frame:\", frame_id)\n",
    "    assert frame_id in extrinsics, \"No pose found for frame \" + frame_id\n",
    "\n",
    "    pose = extrinsics[frame_id]\n",
    "    # pose = np.linalg.inv(pose)\n",
    "\n",
    "    # Convert depth to 3D coordinates in camera coordinates\n",
    "    # u, v = np.meshgrid(np.arange(width), np.arange(height-1, -1, -1))\n",
    "    u, v = np.meshgrid(np.arange(width), np.arange(height))\n",
    "    u = u.flatten()\n",
    "    v = v.flatten()\n",
    "\n",
    "    depth_scale = 1.0 / 10000.0\n",
    "    depth = depth_array.flatten() * depth_scale\n",
    "\n",
    "    # print(depth.shape, np.min(depth), np.max(depth))\n",
    "\n",
    "    X = ((u - cx) * depth / fx)\n",
    "    Y = ((v - cy) * depth / fy)\n",
    "    # Z = -depth\n",
    "    Z = depth\n",
    "\n",
    "    # print(np.min(depth), np.max(depth))\n",
    "\n",
    "    rgb_values = color_array.reshape((-1, 3)) / 255.0\n",
    "    \n",
    "    # Stack the 3D coordinates\n",
    "    points_camera = np.vstack((X, Y, Z, np.ones_like(X)))\n",
    "\n",
    "\n",
    "    n_pose = pose\n",
    "    flip_YZ = np.array([[1, 0, 0, 0],[0, -1, 0, 0],[0, 0, -1, 0],[0, 0, 0, 1]])\n",
    "    n_pose = np.dot(pose, flip_YZ)\n",
    "\n",
    "    # Transform to global coordinates\n",
    "    points_global = np.dot(n_pose, points_camera)\n",
    "    # points_global = np.dot( flip_YZ, points_global)\n",
    "\n",
    "    # Transform points again to camera coordinates and check\n",
    "    if False:\n",
    "        # Extract the transformed X, Y, Z coordinates\n",
    "        R = pose[:3, :3]\n",
    "        t = pose[:3, 3]\n",
    "        W2C = getWorld2View3(R, t)\n",
    "        W2C = np.dot(flip_YZ, W2C)\n",
    "        points_camera_again =  np.dot(W2C,points_global)\n",
    "        pcd = getPCD(points_camera_again, rgb_values)\n",
    "        # print('PCA:', np.shape(points_camera_again), points_camera_again[2, :])\n",
    "        # Transform the points to OpenCV coordinate system\n",
    "        # pcd.transform([[1, 0, 0, 0],[0, -1, 0, 0],[0, 0, -1, 0],[0, 0, 0, 1]])\n",
    "        point_cloud += pcd\n",
    "\n",
    "    if False:\n",
    "        pcd = getPCD(points_camera, rgb_values)\n",
    "        point_cloud += pcd\n",
    "\n",
    "    # Extract the transformed X, Y, Z coordinates\n",
    "    X_global = points_global[0, :] / points_global[3 , :]\n",
    "    Y_global = points_global[1, :] / points_global[3 , :]\n",
    "    Z_global = points_global[2, :] / points_global[3 , :]\n",
    "    # Stack the global coordinates\n",
    "    point_cloud_global = np.vstack((X_global, Y_global, Z_global)).T\n",
    "\n",
    "    # Create an Open3D PointCloud\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    # Set the points in the PointCloud \n",
    "    pcd.points = o3d.utility.Vector3dVector(point_cloud_global)\n",
    "\n",
    "    # print(\"Point Cloud Shape:\", np.shape(point_cloud_global))\n",
    "\n",
    "    # Flip the point cloud\n",
    "    # pcd.transform([[1, 0, 0, 0],[0, -1, 0, 0],[0, 0, -1, 0],[0, 0, 0, 1]])\n",
    "\n",
    "    # Add color to the point cloud\n",
    "    pcd.colors = o3d.utility.Vector3dVector(rgb_values)\n",
    "\n",
    "    # if i == 0:\n",
    "    #     Rpcd = pcd\n",
    "            \n",
    "    # Save the point cloud to a file\n",
    "    ply_file_path = os.path.join(ply_path, frame_id + \".ply\")    \n",
    "    # o3d.io.write_point_cloud(ply_file_path, pcd)\n",
    "\n",
    "    # Merge current point cloud with the overall point cloud\n",
    "    point_cloud += pcd\n",
    "\n",
    "    # Add camera Position\n",
    "    arrow = getArrowMesh()\n",
    "    arrow.transform(pose)\n",
    "    arrows += arrow\n",
    "\n",
    "    # Clear memory\n",
    "    depth_image = None\n",
    "    color_image = None\n",
    "    depth_array = None\n",
    "    color_array = None\n",
    "    pcd = None\n",
    "\n",
    "# Create an Open3D mesh representing coordinate axes\n",
    "axes = o3d.geometry.TriangleMesh.create_coordinate_frame(size=1, origin=[0, 0, 0])\n",
    "\n",
    "geometry_list = [point_cloud, arrows, axes]\n",
    "o3d.visualization.draw_geometries(geometry_list)\n",
    "\n",
    "pcd = None\n",
    "axes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProjectionMatrixTorch(znear, zfar, fovX, fovY):\n",
    "    tanHalfFovY = math.tan((np.radians(fovY) / 2))\n",
    "    tanHalfFovX = math.tan((np.radians(fovX) / 2))\n",
    "\n",
    "    top = tanHalfFovY * znear\n",
    "    bottom = -top\n",
    "    right = tanHalfFovX * znear\n",
    "    left = -right\n",
    "\n",
    "    P = torch.zeros(4, 4)\n",
    "\n",
    "    z_sign = 1.0\n",
    "\n",
    "    P[0, 0] = 2.0 * znear / (right - left)\n",
    "    P[1, 1] = 2.0 * znear / (top - bottom)\n",
    "    P[0, 2] = (right + left) / (right - left)\n",
    "    P[1, 2] = (top + bottom) / (top - bottom)\n",
    "    P[3, 2] = z_sign\n",
    "    P[2, 2] = z_sign * zfar / (zfar - znear)\n",
    "    P[2, 3] = -(zfar * znear) / (zfar - znear)\n",
    "    return P\n",
    "\n",
    "def showRasterizedImageTorch(u,v, colors):\n",
    "    image_width, image_height = width, height\n",
    "    raster = torch.zeros((image_height, image_width, 3), dtype=torch.uint8 , device=colors.device)\n",
    "\n",
    "    # Create Indices\n",
    "    u_long = u.to(torch.long)\n",
    "    v_long = v.to(torch.long)\n",
    "\n",
    "    # print(u_long.min(), u_long.max())\n",
    "\n",
    "    # Store Points and Colors\n",
    "    raster[v_long, u_long] = (colors * 255).to(torch.uint8)\n",
    "    raster = raster.cpu().numpy()\n",
    "    plt.imshow(raster.astype(int))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_id = '0000'\n",
    "ply_file_path = os.path.join(ply_path, frame_id + \".ply\") \n",
    "Rpcd = o3d.io.read_point_cloud(ply_file_path)\n",
    "points = torch.tensor(Rpcd.points, dtype=torch.float32, device='cuda')\n",
    "colors = torch.tensor(Rpcd.colors, dtype=torch.float32, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterize Point Clouds\n",
    "C2W = extrinsics[frame_id]\n",
    "\n",
    "print(frame_id, C2W)\n",
    "print(width, height)\n",
    "\n",
    "# Calculate projection matrix from camera to world\n",
    "R = C2W[:3, :3]\n",
    "t = C2W[:3, 3]\n",
    "W2C = getWorld2View3(R, t)\n",
    "\n",
    "# Flip the Z Axis\n",
    "W2C = np.dot(np.array([[1, 0, 0, 0],[0, -1, 0, 0],[0, 0, -1, 0],[0, 0, 0, 1]]), W2C)\n",
    "\n",
    "image_width, image_height = width, height\n",
    "\n",
    "# cx = rgb_camera_params['cx,cy,fx,fy'][0]\n",
    "# cy = rgb_camera_params['cx,cy,fx,fy'][1]\n",
    "# fx = rgb_camera_params['cx,cy,fx,fy'][2] \n",
    "# fy = rgb_camera_params['cx,cy,fx,fy'][3]\n",
    "\n",
    "# # Normalized intrinsic parameters\n",
    "# fx_norm = 2 * fx / image_width\n",
    "# fy_norm = 2 * fy / image_height\n",
    "# cx_norm = (2 * cx - image_width) / image_width\n",
    "# cy_norm = (2 * cy - image_height) / image_height\n",
    "\n",
    "zNear, zFar = 0.01, 100\n",
    "\n",
    "# # Projection matrix\n",
    "# P1 = np.array([\n",
    "#     [fx_norm, 0, cx_norm, 0],\n",
    "#     [0, fy_norm, cy_norm, 0],\n",
    "#     [0, 0, -(zFar + zNear) / (zFar - zNear), -2 * zFar * zNear / (zFar - zNear)],\n",
    "#     [0, 0, -1, 0]\n",
    "# ])\n",
    "# FullProjection = np.dot(P1, W2C)\n",
    "\n",
    "fovX = rgb_camera_params['hFov']\n",
    "fovY = rgb_camera_params['vFov']\n",
    "\n",
    "print(\"fovX:\", fovX)\n",
    "print(\"fovY:\", fovY)\n",
    "\n",
    "P2 = getProjectionMatrixTorch(zNear, zFar, fovX, fovY)\n",
    "FullProjection = np.dot(P2, W2C)\n",
    "# print(np.dot(P1, W2C))\n",
    "# print(np.dot(FullProjection, W2C))\n",
    "projection_matrix = torch.tensor(FullProjection, dtype=torch.float32, device='cuda')\n",
    "\n",
    "print('W2C:',W2C)\n",
    "print('Projection:', P2)\n",
    "print('FP:',FullProjection)\n",
    "\n",
    "points_homogeneous = torch.cat((points, torch.ones(points.shape[0], 1, device=points.device)), dim=1)\n",
    "\n",
    "view_matrix = torch.tensor(W2C, dtype=torch.float32, device='cuda')\n",
    "view_points = torch.matmul(points_homogeneous, view_matrix.t())\n",
    "print('View Space Extent:', view_points[:,2].min(), view_points[:,2].max())\n",
    "\n",
    "projected_points_homogeneous = torch.matmul(points_homogeneous, projection_matrix.t()) # Clip space coordinates\n",
    "\n",
    "print('Clip Space:', projected_points_homogeneous.shape, projected_points_homogeneous.min(), projected_points_homogeneous.max())\n",
    "\n",
    "# Filter points outside clip space\n",
    "# mask = (projected_points_homogeneous[:, 0] >= -1) & (projected_points_homogeneous[:, 0] < 1) & \\\n",
    "#        (projected_points_homogeneous[:, 1] >= -1) & (projected_points_homogeneous[:, 1] < 1 ) & \\\n",
    "#        (projected_points_homogeneous[:, 2] >= -1) & (projected_points_homogeneous[:, 2] < 1) \n",
    "# projected_points_homogeneous = projected_points_homogeneous[mask]\n",
    "\n",
    "# Clip Space / Homogenous to NDC\n",
    "assert projected_points_homogeneous.shape[1] == 4\n",
    "# Extract x, y, z, w from the tensor\n",
    "x, y, z, w = projected_points_homogeneous[:, 0], projected_points_homogeneous[:, 1], projected_points_homogeneous[:, 2], projected_points_homogeneous[:, 3]\n",
    "x_ndc = x / w\n",
    "y_ndc = y / w\n",
    "z_ndc = z / w\n",
    "projected_points_NDC = torch.stack((x_ndc, y_ndc, z_ndc), dim=1)\n",
    "\n",
    "print('NDC:', projected_points_NDC.shape, projected_points_NDC.min(), projected_points_NDC.max())\n",
    "\n",
    "# NDC to Image Space [-1,1] to [0,1]\n",
    "projected_points_IS = (projected_points_NDC + 1) / 2\n",
    "print('IS:', projected_points_IS.shape, projected_points_IS.min(), projected_points_IS.max())\n",
    "\n",
    "\n",
    "# Filter points outside image space\n",
    "mask = (projected_points_IS[:, 0] >= 0) & (projected_points_IS[:, 0] <= 1) & \\\n",
    "       (projected_points_IS[:, 1] >= 0) & (projected_points_IS[:, 1] <= 1 )  & \\\n",
    "       (projected_points_IS[:, 2] >= 0) & (projected_points_IS[:, 2] <= 1)\n",
    "projected_points_IS = projected_points_IS[mask]\n",
    "\n",
    "points_filtered = points[mask]\n",
    "colors_filtered = colors[mask]\n",
    "\n",
    "print('Values: ', points_filtered.shape, colors_filtered.shape)\n",
    "\n",
    "u = projected_points_IS[:,0] * image_width\n",
    "v = projected_points_IS[:,1] * image_height\n",
    "showRasterizedImageTorch( u, v , colors_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frame_id)\n",
    "print(points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
